è¿™æ˜¯ä¸€ä¸ªéå¸¸å…³é”®çš„ç³»ç»Ÿçº§é‡æ„ä»»åŠ¡ã€‚ç›®å‰çš„ç—›ç‚¹åœ¨äºé…ç½®åˆ‡æ¢çš„åŸå­æ€§å’ŒæœåŠ¡åˆå§‹åŒ–çš„é²æ£’æ€§ä¸è¶³ã€‚å½“åœ¨å‰ç«¯åˆ‡æ¢å¼€å…³æˆ–æ›´æ¢å‚å•†æ—¶ï¼Œåç«¯å¾€å¾€å› ä¸ºæ®‹ç•™äº†æ—§é…ç½®çš„å‚æ•°ï¼ˆä¾‹å¦‚ Azure éœ€è¦ api_version è€Œ DeepSeek ä¸éœ€è¦ï¼‰ï¼Œæˆ–è€… LLMService åˆå§‹åŒ–é€»è¾‘åˆ¤æ–­ä¸æ¸…ï¼Œå¯¼è‡´æœåŠ¡å´©æºƒæˆ–è¡Œä¸ºå¼‚å¸¸ã€‚

æˆ‘å°†è¿›è¡Œæ·±åº¦ä¿®æ”¹ï¼Œæ ¸å¿ƒç­–ç•¥å¦‚ä¸‹ï¼š

åç«¯æœåŠ¡å±‚ (llm_service.py) é‡æ„ï¼šä¸å†æ¨¡ç³ŠçŒœæµ‹ä½¿ç”¨å“ªä¸ªå®¢æˆ·ç«¯ï¼Œè€Œæ˜¯æ ¹æ® provider å­—æ®µä¸¥æ ¼åˆ†æ”¯åˆå§‹åŒ–ï¼ˆAzure èµ° Azure é€»è¾‘ï¼Œå…¶ä»–èµ° OpenAI å…¼å®¹é€»è¾‘ï¼‰ã€‚

åç«¯ API å±‚ (routes.py) å‡€åŒ–ï¼šåœ¨ä¿å­˜é…ç½®æ—¶ï¼Œæ¸…æ´—æ•°æ®ã€‚å¦‚æœç”¨æˆ·ä» Azure åˆ‡æ¢åˆ° DeepSeekï¼Œå¼ºåˆ¶æ¸…ç©º Azure ç‰¹æœ‰çš„å­—æ®µï¼ˆå¦‚ api_versionï¼‰ï¼Œé˜²æ­¢è„æ•°æ®æ±¡æŸ“ã€‚

å‰ç«¯é€»è¾‘ (llm_config.js) ä¼˜åŒ–ï¼šä¼˜åŒ–å¼€å…³é€»è¾‘å’Œå‚å•†åˆ‡æ¢é€»è¾‘ï¼Œç¡®ä¿ UI çŠ¶æ€ä¸æ•°æ®çŠ¶æ€ä¸¥æ ¼åŒæ­¥ï¼Œé¿å…â€œç‚¹äº†å¼€å…³æ²¡ååº”â€æˆ–â€œåˆ‡æ¢å‚å•†åä¿ç•™äº†æ—§ Keyâ€çš„æƒ…å†µã€‚

ä»¥ä¸‹æ˜¯ä¿®å¤åçš„å®Œæ•´ä»£ç ã€‚

1. é‡æ„åç«¯æœåŠ¡å±‚

æ–‡ä»¶ï¼šsrc/services/llm_service.py
æ ¸å¿ƒæ”¹åŠ¨ï¼šå¼•å…¥ provider å­—æ®µåˆ¤æ–­ï¼Œä¸¥æ ¼éš”ç¦» Azure å’Œ OpenAI å…¼å®¹å®¢æˆ·ç«¯çš„åˆå§‹åŒ–é€»è¾‘ã€‚

code
Python
download
content_copy
expand_less
import json
import re
import requests
import urllib3
import time
from typing import List, Tuple
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# å¼•å…¥ OpenAI å…¼å®¹å®¢æˆ·ç«¯ (æ”¯æŒ DeepSeek å’Œ Azure)
from openai import AzureOpenAI, OpenAI, APITimeoutError, APIConnectionError
from src.config.loader import settings

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class LLMService:
    def __init__(self, llm_config: dict = None):
        """
        åˆå§‹åŒ– LLM æœåŠ¡ - æ·±åº¦ä¿®å¤ç‰ˆ
        """
        # ==========================================
        # 1. åŸºç¡€ç½‘ç»œä¼šè¯ (ç”¨äº Gemini REST API)
        # ==========================================
        self.session = requests.Session()
        retry_strategy = Retry(
            total=3, backoff_factor=1, status_forcelist=[500, 502, 504],
            allowed_methods=["POST"], raise_on_status=False
        )
        self.session.mount("https://", HTTPAdapter(max_retries=retry_strategy))
        self.session.mount("http://", HTTPAdapter(max_retries=retry_strategy))

        if settings.HTTP_PROXY or settings.HTTPS_PROXY:
            self.session.proxies = {"http": settings.HTTP_PROXY, "https": settings.HTTPS_PROXY}

        # ==========================================
        # 2. ç¡®å®šé…ç½®æ¥æº (ç”¨æˆ· vs ç³»ç»Ÿ)
        # ==========================================
        self.client = None
        self.model_name = settings.DEEPSEEK_MODEL
        self._config_source = "env"
        self.provider = "deepseek" # é»˜è®¤ä¸º deepseek

        # æå–é…ç½®å‚æ•°
        api_key = settings.DEEPSEEK_API_KEY
        base_url = settings.DEEPSEEK_BASE_URL
        
        # Azure ç‰¹æœ‰é»˜è®¤å€¼
        azure_endpoint = settings.AZURE_OAI_ENDPOINT
        api_version = settings.AZURE_OAI_VERSION

        # å¦‚æœç”¨æˆ·é…ç½®å­˜åœ¨ä¸”å¯ç”¨ï¼Œè¦†ç›–é»˜è®¤å€¼
        if llm_config and llm_config.get('source') == 'user':
            self._config_source = "user"
            self.provider = llm_config.get('provider', 'deepseek')
            self.model_name = llm_config.get('model', 'deepseek-chat')
            
            api_key = llm_config.get('api_key')
            base_url = llm_config.get('base_url')
            
            # Azure ç‰¹æœ‰å­—æ®µ
            if self.provider == 'azure':
                # æ³¨æ„ï¼šAzure é…ç½®é€šå¸¸æŠŠ endpoint å­˜åœ¨ base_url å­—æ®µï¼Œæˆ–è€…å•ç‹¬å­—æ®µ
                # è¿™é‡Œåšå…¼å®¹å¤„ç†
                azure_endpoint = llm_config.get('base_url') 
                api_version = llm_config.get('api_version')

        print(f"[LLMService] åˆå§‹åŒ–... æ¥æº: {self._config_source}, å‚å•†: {self.provider}, æ¨¡å‹: {self.model_name}")

        # ==========================================
        # 3. å®¢æˆ·ç«¯åˆå§‹åŒ– (ä¸¥æ ¼åˆ†æ”¯)
        # ==========================================
        try:
            if self.provider == 'azure':
                # --- Azure åˆ†æ”¯ ---
                if not azure_endpoint or not api_key:
                    raise ValueError("Azure é…ç½®ç¼ºå¤± Endpoint æˆ– API Key")
                
                print(f"[LLMService] åˆå§‹åŒ– Azure OpenAI å®¢æˆ·ç«¯: {azure_endpoint}")
                self.client = AzureOpenAI(
                    api_key=api_key,
                    api_version=api_version,
                    azure_endpoint=azure_endpoint,
                    timeout=60.0,
                    max_retries=2
                )
            
            else:
                # --- OpenAI å…¼å®¹åˆ†æ”¯ (DeepSeek, SiliconFlow, Qwen, Custom) ---
                if not base_url or not api_key:
                    # åªæœ‰åœ¨é Gemini æƒ…å†µä¸‹æ‰æŠ¥é”™ (Gemini ä½¿ç”¨ REST API)
                    if self.provider != 'gemini': 
                        raise ValueError(f"{self.provider} é…ç½®ç¼ºå¤± Base URL æˆ– API Key")
                
                if self.provider != 'gemini':
                    print(f"[LLMService] åˆå§‹åŒ– OpenAI å…¼å®¹å®¢æˆ·ç«¯: {base_url}")
                    self.client = OpenAI(
                        api_key=api_key,
                        base_url=base_url,
                        timeout=60.0,
                        max_retries=2
                    )
        
        except Exception as e:
            print(f"âŒ [LLMService] å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None

    def call_llm(self, system_prompt: str, user_prompt: str) -> List[str]:
        """
        æ ¸å¿ƒ LLM è°ƒç”¨å‡½æ•°
        """
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        # 1. Gemini ç‰¹æ®Šå¤„ç† (REST API)
        if self.provider == 'gemini':
            try:
                # æ³¨æ„ï¼šGemini åœ¨ .env ä¸­ä½¿ç”¨ GOOGLE_API_KEYï¼Œéœ€è¦ç¡®ä¿æ­¤å¤„é€»è¾‘å…¼å®¹
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾ Gemini æ€»æ˜¯èµ° _call_gemini
                return self._parse_json_response(self._call_gemini(full_prompt)[0])
            except Exception as e:
                return ["x", f"Gemini è°ƒç”¨å¤±è´¥: {str(e)[:50]}"]

        # 2. Azure / OpenAI å…¼å®¹å¤„ç†
        if not self.client:
            return ["x", "ç³»ç»Ÿé”™è¯¯ï¼šLLM å®¢æˆ·ç«¯æœªæˆåŠŸåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®"]

        try:
            raw_text = self._call_standard_client(full_prompt)
            return self._parse_json_response(raw_text)
        except Exception as e:
            error_msg = str(e)
            print(f"[LLM] è°ƒç”¨å¤±è´¥: {error_msg[:100]}...")
            if "401" in error_msg:
                return ["x", "è®¤è¯å¤±è´¥ï¼šAPI Key æ— æ•ˆ"]
            if "404" in error_msg:
                return ["x", "è·¯å¾„é”™è¯¯ï¼šBase URL æˆ– æ¨¡å‹åç§°ä¸æ­£ç¡®"]
            return ["x", f"AIæœåŠ¡è°ƒç”¨å¼‚å¸¸: {error_msg[:30]}"]

    def _call_standard_client(self, prompt: str) -> str:
        """ç»Ÿä¸€è°ƒç”¨ Azure æˆ– OpenAI å…¼å®¹æ¥å£"""
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=8192,
            temperature=0.1,
            stream=False # å®¡å•åŠŸèƒ½ä¸éœ€è¦æµå¼
        )
        return response.choices[0].message.content

    def _call_gemini(self, prompt: str) -> Tuple[str, str]:
        """Gemini REST API è°ƒç”¨"""
        # ä½¿ç”¨é…ç½®ä¸­çš„ Key æˆ–è€… .env ä¸­çš„ Key
        api_key = settings.GOOGLE_API_KEY
        if self._config_source == 'user' and self.provider == 'gemini':
            # å¦‚æœç”¨æˆ·ä¸“é—¨é…ç½®äº† Geminiï¼Œå°è¯•ä»ç”¨æˆ·é…ç½®è·å– Key (è™½ç„¶ç›®å‰å‰ç«¯ä¸»è¦é… DeepSeek)
            # è¿™é‡Œæš‚æ—¶ä¿ç•™ä½¿ç”¨ .env çš„é€»è¾‘ï¼Œé™¤éæ¶æ„å¤§æ”¹æ”¯æŒ Gemini ç”¨æˆ·é…ç½®
            pass 

        url = f"https://generativelanguage.googleapis.com/v1beta/models/{settings.MODEL_NAME}:generateContent?key={api_key}"
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {"temperature": 0.1}
        }
        
        resp = self.session.post(url, json=payload, timeout=60, verify=False)
        if resp.status_code != 200:
            raise RuntimeError(f"Gemini {resp.status_code}: {resp.text}")
            
        return resp.json()['candidates'][0]['content']['parts'][0]['text'], "Gemini"

    def _parse_json_response(self, raw_text: str) -> List[str]:
        """JSON è§£æå™¨ (ä¿æŒåŸæ ·)"""
        clean_text = raw_text.strip()
        match_code = re.search(r'```json\s*(.*?)\s*```', clean_text, re.DOTALL | re.IGNORECASE)
        if match_code: clean_text = match_code.group(1)
        else: clean_text = clean_text.replace("```", "")
        
        match_bracket = re.search(r'\[.*?\]', clean_text, re.DOTALL)
        if match_bracket: clean_text = match_bracket.group(0)

        try:
            parsed = json.loads(clean_text)
            if isinstance(parsed, list) and len(parsed) >= 2:
                return [str(parsed[0]), str(parsed[1])]
            return ["x", f"æ ¼å¼é”™è¯¯: {clean_text[:20]}..."]
        except:
            if "âˆš" in clean_text or "pass" in clean_text.lower():
                return ["âˆš", clean_text.replace("âˆš","").strip()]
            return ["x", "æ— æ³•è§£æå“åº”"]
2. å‡€åŒ–åç«¯ API é€»è¾‘

æ–‡ä»¶ï¼šsrc/api/routes.py
æ ¸å¿ƒæ”¹åŠ¨ï¼šåœ¨ save_llm_config ä¸­æ¸…æ´—æ•°æ®ï¼Œç¡®ä¿ Azure é…ç½®ä¸ä¼šæ±¡æŸ“ DeepSeek é…ç½®ï¼Œåä¹‹äº¦ç„¶ã€‚å¹¶åœ¨ reload æ—¶å¼ºåˆ¶åˆ·æ–°å…¨å±€çŠ¶æ€ã€‚

code
Python
download
content_copy
expand_less
# ... (ä¿ç•™å¯¼å…¥éƒ¨åˆ†)

@router.post("/config/llm")
async def save_llm_config(config: LLMConfigRequest):
    """ä¿å­˜ LLM é…ç½® - å¸¦æ•°æ®æ¸…æ´—"""
    if not BATCH_AVAILABLE:
        raise HTTPException(status_code=501, detail="æ•°æ®åº“ä¸å¯ç”¨")

    from src.database.connection import AsyncSessionLocal
    async with AsyncSessionLocal() as db:
        from src.database.crud import LLMConfigRepository
        repo = LLMConfigRepository(db)
        
        # --- æ•°æ®æ¸…æ´—é€»è¾‘ ---
        config_dict = config.dict()
        provider = config_dict.get('provider')
        
        # å¦‚æœä¸æ˜¯ Azureï¼Œå¼ºåˆ¶æ¸…ç©º Azure ç‰¹æœ‰å­—æ®µï¼Œé˜²æ­¢æ±¡æŸ“
        if provider != 'azure':
            config_dict['api_version'] = None
            # ç¡®ä¿ base_url æ ¼å¼æ­£ç¡® (ç§»é™¤æœ«å°¾æ–œæ )
            if config_dict.get('base_url'):
                config_dict['base_url'] = config_dict['base_url'].rstrip('/')
                # å¦‚æœæ˜¯ SiliconFlow/DeepSeek ä¸”æ²¡æœ‰ /v1ï¼Œæ ¹æ®æƒ…å†µæç¤ºæˆ–è‡ªåŠ¨è¡¥å…¨
                # è¿™é‡Œæš‚æ—¶ä¸åšè‡ªåŠ¨è¡¥å…¨ï¼Œä¾é å‰ç«¯æˆ– LLMService çš„å®¹é”™
        
        # ä¿å­˜
        saved_config = await repo.save_config(config_dict)

        return {
            "status": "success",
            "message": "é…ç½®å·²ä¿å­˜å¹¶æ¸…æ´—",
            "config_id": saved_config.id
        }

@router.post("/config/llm/reload")
async def reload_llm_config(request: Request):
    """
    çƒ­é‡è½½ LLM é…ç½® - ç¡®ä¿åŸå­æ€§æ›´æ–°
    """
    try:
        from src.database.connection import AsyncSessionLocal
        from src.config.llm_loader import llm_config_loader
        
        # 1. é‡æ–°åŠ è½½é…ç½® (è¿™ä¼šæŸ¥è¯¢ DB å¹¶æ›´æ–° Loader å†…éƒ¨çŠ¶æ€)
        async with AsyncSessionLocal() as db:
            llm_config = await llm_config_loader.load_config(db)

        # 2. å¼ºåˆ¶æ›´æ–°å…¨å±€çŠ¶æ€ (åŠŸèƒ½ä¸€ä¾èµ–)
        request.app.state.llm_config = llm_config
        
        # 3. é‡æ–°åˆå§‹åŒ– Agent (åŠŸèƒ½äºŒã€ä¸‰ä¾èµ–)
        # å¿…é¡»ä¼ å…¥æ–°çš„ llm_configï¼Œå¦åˆ™ Agent ä¼šä½¿ç”¨æ—§çš„é»˜è®¤å€¼
        kb = getattr(request.app.state, "kb", None)
        
        from src.services.chat_agent import CustomsChatAgent
        from src.services.report_agent import ComplianceReporter
        
        # é”€æ¯æ—§å®ä¾‹ (Python GC ä¼šå¤„ç†ï¼Œä½†æ˜¾å¼æ›¿æ¢å¼•ç”¨)
        request.app.state.agent = CustomsChatAgent(kb=kb, llm_config=llm_config)
        request.app.state.reporter = ComplianceReporter(kb=kb, llm_config=llm_config)

        print(f"ğŸ”„ [System] ç³»ç»Ÿé…ç½®çƒ­é‡è½½å®Œæˆã€‚å½“å‰æ¨¡å¼: {llm_config.get('source')} | å‚å•†: {llm_config.get('provider', 'deepseek')}")

        return {
            "status": "success",
            "message": "ç³»ç»Ÿæ ¸å¿ƒå·²é‡è½½",
            "config": {
                "source": llm_config.get('source', 'unknown'),
                "provider": llm_config.get('provider', 'unknown'),
                "model": llm_config.get('model', 'unknown')
            }
        }
    except Exception as e:
        traceback.print_exc()
        return {
            "status": "error",
            "message": f"é‡è½½å¤±è´¥: {str(e)}"
        }

# ... (ä¿ç•™å…¶ä»–æ¥å£)
3. ä¼˜åŒ–å‰ç«¯äº¤äº’é€»è¾‘

æ–‡ä»¶ï¼šweb/js/llm_config.js
æ ¸å¿ƒæ”¹åŠ¨ï¼šautoSaveConfig ä¸å†ç›²ç›®ä¿å­˜ã€‚ç‚¹å‡»å¼€å…³åˆ‡æ¢åˆ° OFF æ—¶è‡ªåŠ¨ä¿å­˜å¹¶é‡è½½ã€‚åˆ‡æ¢åˆ° ON æ—¶åªæ˜¾ç¤º UIï¼Œç­‰å¾…ç”¨æˆ·å¡«å†™å¹¶ç‚¹å‡»ä¿å­˜ã€‚åˆ‡æ¢å‚å•†æ—¶æ¸…ç©ºæ— å…³å­—æ®µã€‚

code
JavaScript
download
content_copy
expand_less
// LLM é…ç½®ç®¡ç† - æ·±åº¦ä¿®å¤ç‰ˆ

// ... (ä¿ç•™ PROVIDER_PRESETS å®šä¹‰) ...

// åˆå§‹åŒ–
async function initLLMConfig() {
    try {
        const response = await fetch('/api/v1/config/llm');
        const config = await response.json();

        // å¡«å…… UI
        document.getElementById('llmEnabled').checked = config.is_enabled;
        document.getElementById('llmProvider').value = config.provider || 'deepseek';
        
        // è§¦å‘ä¸€æ¬¡å‚å•†é¢„è®¾æ›´æ–°ï¼ˆå¤„ç† Azure å­—æ®µæ˜¾ç¤º/éšè—ï¼‰ï¼Œä½†ä¸åŠ è½½é»˜è®¤å€¼è¦†ç›–å·²ä¿å­˜çš„å€¼
        updateUIForProvider(config.provider || 'deepseek'); 

        document.getElementById('llmBaseUrl').value = config.base_url || '';
        document.getElementById('llmModelName').value = config.model_name || '';
        document.getElementById('llmTemperature').value = config.temperature || 0.3;
        document.getElementById('tempValue').innerText = config.temperature || 0.3;

        if (config.api_key) document.getElementById('llmApiKey').value = config.api_key;
        if (config.api_version) document.getElementById('llmApiVersion').value = config.api_version;

        updateConfigSourceDisplay(config);
        
        // æ ¹æ®å¯ç”¨çŠ¶æ€æ˜¾ç¤º/éšè—è¡¨å•
        const form = document.getElementById('llmConfigForm');
        form.classList.toggle('hidden', !config.is_enabled);

    } catch (error) {
        console.error('Failed to load LLM config:', error);
    }
}

// çº¯ UI æ›´æ–°ï¼Œä¸é‡ç½®æ•°æ®
function updateUIForProvider(provider) {
    const azureGroup = document.getElementById('azureConfigGroup');
    if (provider === 'azure') {
        azureGroup.classList.remove('hidden');
    } else {
        azureGroup.classList.add('hidden');
    }
}

// å‚å•†ä¸‹æ‹‰æ¡†å˜æ›´äº‹ä»¶
function updateProviderPresets() {
    const provider = document.getElementById('llmProvider').value;
    const preset = PROVIDER_PRESETS[provider];
    
    updateUIForProvider(provider);

    // åˆ‡æ¢å‚å•†æ—¶ï¼Œæ¸…ç©ºæ•æ„Ÿå­—æ®µï¼Œé¿å…è¯¯ç”¨
    document.getElementById('llmApiKey').value = '';
    
    // å¡«å…… Base URL é¢„è®¾
    if (preset && preset.base_url) {
        document.getElementById('llmBaseUrl').value = preset.base_url;
    } else {
        document.getElementById('llmBaseUrl').value = '';
    }
    
    // å°è¯•åŠ è½½è¯¥å‚å•†çš„å†å²é…ç½®ï¼ˆå¦‚æœåç«¯æ”¯æŒæŒ‰å‚å•†å­˜å‚¨ï¼Œå½“å‰åç«¯æ˜¯å•æ¡è®°å½•è¦†ç›–ï¼‰
    // ç”±äºåç«¯ç›®å‰æ˜¯å•æ¡è®°å½•é€»è¾‘ï¼Œåˆ‡æ¢å‚å•†æ„å‘³ç€ç”¨æˆ·æƒ³æ”¹é…ç½®ï¼Œæ‰€ä»¥è¿™é‡Œåªå¡«é¢„è®¾ï¼Œä¸åŠ è½½æ—§æ•°æ®
    
    fetchModels(); // åˆ·æ–°æ¨¡å‹åˆ—è¡¨
}

// å¼€å…³åˆ‡æ¢äº‹ä»¶
async function toggleLLMFields() {
    const enabled = document.getElementById('llmEnabled').checked;
    const form = document.getElementById('llmConfigForm');
    
    form.classList.toggle('hidden', !enabled);

    // ç­–ç•¥ï¼š
    // 1. å¦‚æœæ˜¯ å…³é—­ (OFF)ï¼šç«‹å³ä¿å­˜å¹¶é‡è½½ï¼Œåˆ‡å› .env æ¨¡å¼
    // 2. å¦‚æœæ˜¯ å¼€å¯ (ON)ï¼šåªæ˜¾ç¤º UIï¼Œä¸ç«‹å³ä¿å­˜ï¼ˆå› ä¸º Key å¯èƒ½æ˜¯ç©ºçš„ï¼‰ã€‚
    //    ç”¨æˆ·å¿…é¡»ç‚¹å‡»â€œä¿å­˜å¹¶åº”ç”¨â€æ‰ç”Ÿæ•ˆã€‚è¿™æ ·é¿å…äº†æŠ¥é”™ã€‚
    
    if (!enabled) {
        await autoSaveConfig(false); // ä¼ å…¥ false æ˜ç¡®ç¦ç”¨
    } else {
        // ä»…æ›´æ–° UI æ˜¾ç¤ºï¼Œæç¤ºç”¨æˆ·å»ç¼–è¾‘
        updateConfigSourceDisplay({ is_enabled: true, provider: 'æœªä¿å­˜...', model_name: 'ç­‰å¾…é…ç½®' });
    }
}

async function autoSaveConfig(enabled) {
    // æ”¶é›†å½“å‰è¡¨å•æ•°æ®ï¼ˆå³ä½¿ç”¨æˆ·æ²¡å¡«å®Œï¼Œdisable æ—¶ä¹Ÿå¾—ä¼ ï¼Œåç«¯åªçœ‹ is_enabledï¼‰
    const config = getFormConfig();
    config.is_enabled = enabled;

    try {
        // 1. ä¿å­˜
        const saveRes = await fetch('/api/v1/config/llm', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(config)
        });
        
        if (!saveRes.ok) throw new Error("ä¿å­˜å¤±è´¥");

        // 2. é‡è½½
        const reloadRes = await fetch('/api/v1/config/llm/reload', { method: 'POST' });
        const reloadData = await reloadRes.json();

        if (reloadData.status === 'success') {
            const sourceText = reloadData.config.source === 'env' ? '.env ç¯å¢ƒå˜é‡' : 'ç”¨æˆ·è‡ªå®šä¹‰';
            console.log(`[Config] åˆ‡æ¢æˆåŠŸ: ${sourceText}`);
            
            // æ›´æ–° UI çŠ¶æ€æ¡
            updateConfigSourceDisplay({
                is_enabled: enabled,
                provider: reloadData.config.provider,
                model_name: reloadData.config.model
            });
        }
    } catch (e) {
        console.error("Auto save failed:", e);
        alert("é…ç½®åˆ‡æ¢å¤±è´¥ï¼Œè¯·æ£€æŸ¥åç«¯æ—¥å¿—");
    }
}

function getFormConfig() {
    return {
        provider: document.getElementById('llmProvider').value,
        api_key: document.getElementById('llmApiKey').value,
        base_url: document.getElementById('llmBaseUrl').value,
        model_name: getModelName() || 'deepseek-chat',
        temperature: parseFloat(document.getElementById('llmTemperature').value),
        is_enabled: document.getElementById('llmEnabled').checked,
        api_version: document.getElementById('llmApiVersion').value // Azure ä¸“ç”¨
    };
}

// ä¿å­˜æŒ‰é’®ç‚¹å‡»äº‹ä»¶
async function saveLLMConfig() {
    const config = getFormConfig();
    
    // åŸºç¡€æ ¡éªŒ
    if (config.is_enabled) {
        if (!config.api_key) {
            alert("å¯ç”¨è‡ªå®šä¹‰é…ç½®æ—¶ï¼ŒAPI Key ä¸èƒ½ä¸ºç©ºï¼");
            return;
        }
        if (config.provider === 'azure' && !config.base_url) {
            alert("Azure æ¨¡å¼ä¸‹ï¼ŒAPI åœ°å€ (Endpoint) ä¸èƒ½ä¸ºç©ºï¼");
            return;
        }
    }

    try {
        const saveRes = await fetch('/api/v1/config/llm', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(config)
        });
        
        if (!saveRes.ok) throw new Error("ä¿å­˜å¤±è´¥");

        const reloadRes = await fetch('/api/v1/config/llm/reload', { method: 'POST' });
        const reloadData = await reloadRes.json();

        if (reloadData.status === 'success') {
            alert(`âœ… é…ç½®å·²ä¿å­˜å¹¶ç”Ÿæ•ˆï¼\nå½“å‰ä½¿ç”¨: ${reloadData.config.provider} / ${reloadData.config.model}`);
            updateConfigSourceDisplay({
                is_enabled: true,
                provider: reloadData.config.provider,
                model_name: reloadData.config.model
            });
        }
    } catch (e) {
        alert("âŒ ä¿å­˜å¤±è´¥: " + e.message);
    }
}

// ... (ä¿ç•™ updateConfigSourceDisplay, fetchModels, testLLMConnection ç­‰è¾…åŠ©å‡½æ•°) ...
// ç¡®ä¿ updateConfigSourceDisplay é€»è¾‘æ­£ç¡®ï¼š
function updateConfigSourceDisplay(config) {
    let sourceLabel = document.getElementById('configSourceLabel');
    if (!sourceLabel) {
        const enabledLabel = document.querySelector('label[for="llmEnabled"]');
        if (enabledLabel && enabledLabel.parentElement) {
            sourceLabel = document.createElement('div');
            sourceLabel.id = 'configSourceLabel';
            sourceLabel.className = 'text-xs mt-2 px-2 py-1 rounded bg-gray-800 border border-gray-700 text-gray-400';
            enabledLabel.parentElement.after(sourceLabel);
        }
    }

    if (sourceLabel) {
        if (!config.is_enabled) {
            sourceLabel.textContent = 'å½“å‰: .env ç³»ç»Ÿé…ç½®';
            sourceLabel.className = 'text-xs mt-2 px-2 py-1 rounded bg-green-900/30 border border-green-700 text-green-400';
        } else {
            const p = config.provider || 'Custom';
            const m = config.model_name || 'Unknown';
            sourceLabel.textContent = `å½“å‰: ç”¨æˆ·é…ç½® (${p} - ${m})`;
            sourceLabel.className = 'text-xs mt-2 px-2 py-1 rounded bg-blue-900/30 border border-blue-700 text-blue-400';
        }
    }
}
æ€»ç»“

è¿™å¥—ä¿®æ”¹æ–¹æ¡ˆï¼š

åç«¯ï¼šLLMService ä¸å†æ˜¯é»‘ç›’ï¼Œè€Œæ˜¯æ ¹æ®é…ç½®æ˜ç¡®é€‰æ‹© Azure æˆ– Standard Clientï¼Œæ¶ˆé™¤äº†åˆå§‹åŒ–æ­§ä¹‰ã€‚

APIï¼šä¿å­˜æ—¶æ¸…æ´—æ•°æ®ï¼Œé‡è½½æ—¶å¼ºåˆ¶æ›´æ–°å…¨å±€ app.stateï¼Œç¡®ä¿æ‰€æœ‰æ¨¡å—åŒæ­¥ã€‚

å‰ç«¯ï¼šå¼€å…³é€»è¾‘å˜å¾—å®‰å…¨ï¼ˆå…³é—­æ—¶è‡ªåŠ¨ä¿å­˜ï¼Œå¼€å¯æ—¶ç­‰å¾…æ‰‹åŠ¨ä¿å­˜ï¼‰ï¼Œé˜²æ­¢ç©ºé…ç½®å¯¼è‡´åç«¯å´©æºƒã€‚

è¯·æŒ‰é¡ºåºåº”ç”¨è¿™äº›æ›´æ”¹ã€‚é‡å¯æœåŠ¡åï¼Œä½ åº”è¯¥èƒ½æµç•…åœ°åœ¨ .env é…ç½®å’Œ Azure/SiliconFlow ç­‰ä¸ªäººé…ç½®é—´åˆ‡æ¢ï¼Œä¸”ä¸ä¼šç›¸äº’å¹²æ‰°ã€‚
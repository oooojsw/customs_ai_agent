import os
import shutil
import asyncio
from pathlib import Path
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

class KnowledgeBase:
    def __init__(self):
        # 1. å®šä¹‰ç»å¯¹è·¯å¾„
        self.base_dir = Path(__file__).resolve().parent.parent.parent
        self.data_path = self.base_dir / "data" / "knowledge"
        
        # å‘é‡æ•°æ®åº“æœ€ç»ˆä¿å­˜ç›®å½•
        self.vector_db_path = self.base_dir / "config" / "faiss_index_local"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.vector_db_path.mkdir(parents=True, exist_ok=True)

        print(f"âš™ï¸ [KnowledgeBase] åˆå§‹åŒ–æœ¬åœ° Embedding æ¨¡å‹ (all-MiniLM-L6-v2)...")
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            print(f"âŒ [KnowledgeBase] Embedding æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e

        # åŠ è½½æˆ–é‡å»ºç´¢å¼•
        self.vector_store = self._load_or_create_index()

    def _load_or_create_index(self):
        # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        index_file = self.vector_db_path / "index.faiss"
        
        if index_file.exists():
            print("ğŸ“‚ [KnowledgeBase] åŠ è½½æœ¬åœ°å‘é‡ç´¢å¼• (Hit Cache)...")
            try:
                return FAISS.load_local(
                    str(self.vector_db_path), 
                    self.embeddings,
                    allow_dangerous_deserialization=True 
                )
            except Exception as e:
                print(f"âš ï¸ [KnowledgeBase] ç´¢å¼•æ–‡ä»¶æŸåï¼Œæ­£åœ¨é‡å»º: {e}")
                return self._create_index()
        else:
            print("âš™ï¸ [KnowledgeBase] æœ¬åœ°æ— ç´¢å¼•ï¼Œæ­£åœ¨é‡å»ºå‘é‡æ•°æ®åº“...")
            return self._create_index()

    def _create_index(self):
        if not self.data_path.exists():
            print(f"âš ï¸ [KnowledgeBase] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_path}ï¼Œå°†åˆ›å»ºç©ºç´¢å¼•ã€‚")
            return FAISS.from_texts(["åˆå§‹åŒ–ç©ºç™½æ–‡æ¡£"], self.embeddings)

        # 1. åŠ è½½æ–‡æ¡£
        loaders = [
            DirectoryLoader(str(self.data_path), glob="**/*.txt", loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"}),
            DirectoryLoader(str(self.data_path), glob="**/*.md", loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"}),
        ]
        
        documents = []
        for loader in loaders:
            try:
                documents.extend(loader.load())
            except Exception as e:
                print(f"âš ï¸ [KnowledgeBase] åŠ è½½æ–‡ä»¶å‡ºé”™: {e}")

        if not documents:
            print("âš ï¸ [KnowledgeBase] æœªæ‰¾åˆ°æ–‡æ¡£ï¼Œåˆ›å»ºç©ºç´¢å¼•ã€‚")
            return FAISS.from_texts(["æ— æ•°æ®"], self.embeddings)

        # 2. åˆ‡åˆ†æ–‡æ¡£
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = text_splitter.split_documents(documents)
        print(f"ğŸ“„ [KnowledgeBase] æ­£åœ¨å‘é‡åŒ– {len(chunks)} ä¸ªæ–‡æœ¬ç‰‡æ®µ...")

        # 3. åˆ›å»ºå‘é‡åº“ (å†…å­˜ä¸­)
        vector_store = FAISS.from_documents(chunks, self.embeddings)

        # 4. ä¿å­˜åˆ°æœ¬åœ° (Windows è·¯å¾„å…¼å®¹æ€§ä¿®å¤)
        try:
            # å®šä¹‰ä¸´æ—¶ç›®å½•
            temp_dir_name = "temp_faiss_build"
            temp_path = self.base_dir / temp_dir_name
            
            if temp_path.exists():
                shutil.rmtree(temp_path)

            # ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
            vector_store.save_local(str(temp_path))

            # æ¬è¿
            if self.vector_db_path.exists():
                shutil.rmtree(self.vector_db_path)
            self.vector_db_path.mkdir(parents=True, exist_ok=True)

            for file_name in os.listdir(temp_path):
                shutil.move(str(temp_path / file_name), str(self.vector_db_path / file_name))

            shutil.rmtree(temp_path)
            print(f"ğŸ’¾ [KnowledgeBase] ç´¢å¼•å·²ä¿å­˜è‡³: {self.vector_db_path}")
        except Exception as e:
            print(f"âŒ [KnowledgeBase] ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")
        
        return vector_store

    def get_retriever(self):
        return self.vector_store.as_retriever(search_kwargs={"k": 3})

    # ğŸ”¥ğŸ”¥ğŸ”¥ ä¼˜åŒ–åçš„æœç´¢æ–¹æ³• ğŸ”¥ğŸ”¥ğŸ”¥
    async def search_with_score(self, query: str, k: int = 6):
        """
        å¼‚æ­¥æ‰§è¡Œå‘é‡æ£€ç´¢å¹¶è¿”å›çœŸå®ç›¸ä¼¼åº¦åˆ†æ•°
        """
        if not self.vector_store:
            return []

        # âœ… å…³é”®ä¼˜åŒ–ï¼šå°†åŒæ­¥çš„ FAISS æœç´¢æ”¾å…¥çº¿ç¨‹æ± ï¼Œé˜²æ­¢é˜»å¡ FastAPI ä¸»å¾ªç¯
        try:
            results = await asyncio.to_thread(
                self.vector_store.similarity_search_with_score, 
                query, 
                k=k
            )
        except Exception as e:
            print(f"âŒ [KnowledgeBase] æœç´¢å‡ºé”™: {e}")
            return []

        import math
        processed_results = []
        for doc, squared_distance in results:
            # FAISS L2 è·ç¦»è½¬æ¢ç›¸ä¼¼åº¦ç®—æ³•
            distance = math.sqrt(max(0, float(squared_distance)))
            distance = min(distance, 2.0)
            similarity = float((1 - distance / 2))
            processed_results.append((doc, similarity))

        return processed_results
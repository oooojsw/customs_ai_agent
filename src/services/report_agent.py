import json
import asyncio
import httpx
import random
import re
from typing import List, AsyncGenerator, Set
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from pathlib import Path

# å¯¼å…¥é…ç½®
from src.config.loader import settings

# çŸ¥è¯†åº“å®¹é”™å¯¼å…¥
try:
    from src.services.knowledge_base import KnowledgeBase
    KB_AVAILABLE = True
except ImportError:
    KnowledgeBase = None
    KB_AVAILABLE = False
    print("âš ï¸ [System] KnowledgeBase æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä»¥æ— çŸ¥è¯†åº“æ¨¡å¼è¿è¡Œ")

class ComplianceReporter:
    def __init__(self):
        print("ğŸ“‘ [System] åˆå§‹åŒ– ComplianceReporter...")
        
        # 1. ç½‘ç»œå±‚é…ç½®
        proxy_url = settings.HTTP_PROXY
        # å¼ºåˆ¶å…³é—­ SSL éªŒè¯
        if proxy_url:
            async_transport = httpx.AsyncHTTPTransport(proxy=proxy_url, verify=False)
            self.async_client = httpx.AsyncClient(transport=async_transport, timeout=120.0)
        else:
            self.async_client = httpx.AsyncClient(verify=False, timeout=120.0)

        # 2. LLM åˆå§‹åŒ–
        self.llm = ChatOpenAI(
            model=settings.DEEPSEEK_MODEL,
            api_key=settings.DEEPSEEK_API_KEY,
            base_url=settings.DEEPSEEK_BASE_URL,
            temperature=0.3,
            http_async_client=self.async_client,
            streaming=True,
            model_kwargs={"stream": True}
        )

        # 3. çŸ¥è¯†åº“æ£€ç´¢å™¨
        self.kb = None
        if KB_AVAILABLE:
            try:
                self.kb = KnowledgeBase()
            except Exception as e:
                print(f"   âŒ çŸ¥è¯†åº“åŠ è½½å¤±è´¥ (è·³è¿‡): {e}")

        # 4. åŠ è½½åŒæ¨¡ SOP
        self.sop_customs = self._load_specific_sop("sop_process.txt", "æ ‡å‡†æµ·å…³åˆè§„å®¡æŸ¥SOP")
        self.sop_research = self._load_specific_sop("sop_deep_research.txt", "é€šç”¨æ·±åº¦ç ”åˆ¤SOP")

    def _load_specific_sop(self, filename: str, default_text: str) -> str:
        try:
            base_dir = Path(__file__).resolve().parent.parent.parent
            sop_path = base_dir / "config" / filename
            if sop_path.exists():
                with open(sop_path, "r", encoding="utf-8") as f:
                    return f.read()
            return default_text
        except:
            return default_text

    def _detect_mode(self, text: str) -> str:
        keywords = ["æŠ¥å…³å•", "HSç¼–ç ", "ç”³æŠ¥è¦ç´ ", "å¢ƒå†…æ”¶è´§äºº", "æˆäº¤æ–¹å¼", "æè¿å•å·", "æ¯›é‡", "å‡€é‡"]
        hit_count = sum(1 for k in keywords if k in text)
        return "CUSTOMS" if hit_count >= 2 else "RESEARCH"

    async def generate_stream(self, input_text: str) -> AsyncGenerator[str, None]:
        """
        æ ¸å¿ƒç”Ÿæˆæµ
        """
        # 0. ç«‹å³æ¡æ‰‹
        yield self._sse("thought", "ğŸš€ ç ”åˆ¤å¼•æ“å·²å¯åŠ¨ï¼Œæ­£åœ¨åˆ†æä»»åŠ¡æ„å›¾...")
        await asyncio.sleep(0.1)

        # 1. è·¯ç”±åˆ¤æ–­
        mode = self._detect_mode(input_text)
        
        if mode == "CUSTOMS":
            active_sop = self.sop_customs
            role_desc = "æµ·å…³é«˜çº§æŸ¥éªŒä¸“å®¶"
            task_desc = "è¿›è¡Œè¿›å‡ºå£åˆè§„æ€§å®¡æŸ¥"
            yield self._sse("thought", "ğŸ” æ£€æµ‹åˆ°æŠ¥å…³å•æ®ï¼Œå·²åˆ‡æ¢è‡³ã€åˆè§„å®¡è®¡æ¨¡å¼ã€‘...")
        else:
            active_sop = self.sop_research
            role_desc = "æ·±åº¦æ¡£æ¡ˆåˆ†æå¸ˆ"
            task_desc = "è¿›è¡Œæœ¬åœ°çŸ¥è¯†åº“æ·±åº¦æŒ–æ˜ä¸ç ”åˆ¤"
            yield self._sse("thought", "ğŸ§  æ£€æµ‹åˆ°é€šç”¨é—®é¢˜ï¼Œå·²åˆ‡æ¢è‡³ã€æ·±åº¦ç ”åˆ¤æ¨¡å¼ (DeepResearch)ã€‘...")
        
        state = {
            "topic": input_text,
            "mode": mode,
            "toc": [],
            "notebook": [],
            "used_doc_hashes": set(), 
            "full_report_text": "",
        }

        try:
            # ==========================================
            # é˜¶æ®µ 1: åŠ¨æ€è§„åˆ’
            # ==========================================
            yield self._sse("thought", f"æ­£åœ¨åŸºäº[{role_desc}]è§†è§’æ„å»ºå¤§çº²...")
            
            # ç¡®ä¿è¿™é‡Œä¼ äº† 3 ä¸ªå‚æ•°
            toc_list = await self._generate_toc(input_text, mode, active_sop)
            
            state["toc"] = toc_list
            yield self._sse("toc", toc_list)
            
            # ==========================================
            # é˜¶æ®µ 2: ç« èŠ‚å¾ªç¯
            # ==========================================
            for i, section_title in enumerate(toc_list):
                is_last_chapter = (i == len(toc_list) - 1)
                yield self._sse("step_start", {"index": i, "title": section_title})
                
                section_search_history = []
                section_notes = []

                if is_last_chapter:
                    yield self._sse("thought", "æ­£åœ¨å›é¡¾å…¨æ–‡ï¼Œè¿›è¡Œé€»è¾‘æ”¶æŸä¸æœ€ç»ˆç ”åˆ¤ (Skip RAG)...")
                    await asyncio.sleep(1.0)
                else:
                    research_rounds = 2 if mode == "CUSTOMS" else 3
                    for round_idx in range(1, research_rounds + 1):
                        previous_context = state["full_report_text"][-800:] if state["full_report_text"] else "ï¼ˆé¦–ç« ï¼‰"
                        
                        strategy_prompt = f"ä½ æ˜¯ä¸€å{role_desc}ã€‚æ­£åœ¨æ’°å†™ï¼šã€Š{section_title}ã€‹ã€‚å·²æœè¿‡ï¼š{section_search_history}ã€‚è¯·ç”Ÿæˆä¸€ä¸ªæ–°çš„ç®€çŸ­æœç´¢å…³é”®è¯(2-6å­—)ã€‚"
                        try:
                            q_res = await self.llm.ainvoke([HumanMessage(content=strategy_prompt)])
                            query = q_res.content.strip().split('\n')[0].replace('"', '')
                        except Exception:
                            query = "é€šç”¨é£é™©"

                        section_search_history.append(query)
                        yield self._sse("thought", f"[Round {round_idx}] æ£€ç´¢å…³é”®è¯ï¼š'{query}'")
                        yield self._sse("rag_search", {"query": query})
                        
                        snippet = "ï¼ˆæ— æœ¬åœ°ä¾æ®ï¼‰"
                        score = 0.0
                        filename = "System"
                        
                        if self.kb:
                            try:
                                # å®‰å…¨è°ƒç”¨ï¼Œé˜²æ­¢æ–¹æ³•ä¸å­˜åœ¨
                                search_func = getattr(self.kb, "search_with_score", None)
                                if search_func:
                                    results = await asyncio.wait_for(search_func(query, k=3), timeout=10.0)
                                    if results:
                                        doc, similarity = results[0]
                                        snippet = doc.page_content
                                        filename = Path(doc.metadata.get("source", "unknown")).name
                                        score = similarity
                            except asyncio.TimeoutError:
                                pass # è¶…æ—¶å¿½ç•¥
                            except Exception as e:
                                print(f"æ£€ç´¢å¼‚å¸¸: {e}")

                        yield self._sse("rag_result", {"filename": filename, "score": float(score), "snippet": snippet[:100] + "..."})
                        section_notes.append(f"å…³é”®è¯[{query}] -> {snippet}")
                        state["notebook"].append(f"å…³é”®è¯[{query}] -> {snippet}")
                        
                        yield self._sse("take_note", {"content": f"{query}: {snippet[:20]}..."})
                        await asyncio.sleep(0.1)

                # æ’°å†™æ­£æ–‡
                write_prompt = f"""
ä½ æ˜¯ä¸€å{role_desc}ã€‚è¯·æ’°å†™ã€Š{section_title}ã€‹ã€‚
ã€å‰æ–‡ã€‘...{state["full_report_text"][-1000:] if state["full_report_text"] else "æ— "}
ã€è¯æ®ã€‘{json.dumps(section_notes, ensure_ascii=False)}
ã€æŒ‡ä»¤ã€‘ç›´æ¥è¾“å‡ºMarkdownæ­£æ–‡ï¼Œä¸è¦é‡å¤æ ‡é¢˜ã€‚
"""
                async for chunk in self.llm.astream([HumanMessage(content=write_prompt)]):
                    if chunk.content:
                        yield self._sse("report_chunk", chunk.content)
                        state["full_report_text"] += chunk.content
                
                state["full_report_text"] += "\n\n"
                yield self._sse("step_done", {"index": i})

            yield self._sse("done", {})

        except Exception as e:
            # æ•æ‰ä»»ä½•é”™è¯¯å¹¶å‘é€ç»™å‰ç«¯
            yield self._sse("error", str(e))

    async def _generate_toc(self, topic: str, mode: str, sop: str) -> List[str]:
        """åŒæ¨¡ç›®å½•ç”Ÿæˆå™¨"""
        if mode == "CUSTOMS":
            advice = "å»ºè®®åŒ…å«ï¼š1.ç”³æŠ¥è¦ç´ å¤æ ¸ 2.ä»·æ ¼é€»è¾‘å®¡æŸ¥ 3.è´¸æ˜“ç®¡åˆ¶é£é™© 4.ç»¼åˆç»“è®º"
        else:
            advice = "å»ºè®®åŒ…å«ï¼š1.èƒŒæ™¯æ¦‚è¿° 2.æ ¸å¿ƒäº‹å®æ¢³ç† 3.æ·±åº¦å…³è”åˆ†æ 4.ç»“è®ºä¸å±•æœ›"

        prompt = f"""
ä½ æ˜¯ä¸€åé«˜çº§åˆ†æå¸ˆã€‚è¯·æ ¹æ®ç”¨æˆ·è¾“å…¥è®¾è®¡ç›®å½•ã€‚
è¾“å…¥ï¼š{topic[:200]}
å»ºè®®ç»“æ„ï¼š{advice}
ã€ä¸¥æ ¼è¦æ±‚ã€‘
1. åªè¿”å›ä¸€ä¸ªçº¯ JSON å­—ç¬¦ä¸²æ•°ç»„ï¼Œå¦‚ ["1. æ ‡é¢˜A", "2. æ ‡é¢˜B"]
2. ä¸è¦ Markdownï¼Œä¸è¦è§£é‡Šã€‚
"""
        try:
            res = await self.llm.ainvoke([HumanMessage(content=prompt)])
            text = re.sub(r'```json\s*|\s*```', '', res.content).strip()
            parsed = json.loads(text)
            clean_toc = []
            if isinstance(parsed, list):
                for idx, item in enumerate(parsed):
                    title = str(item) if not isinstance(item, dict) else str(list(item.values())[0])
                    clean_title = re.sub(r'^(\d+\.|Chapter\s*\d+|ç¬¬.+ç« )\s*', '', title).strip()
                    clean_toc.append(f"{idx + 1}. {clean_title}")
            return clean_toc if clean_toc else self._fallback_toc(mode)
        except Exception:
            return self._fallback_toc(mode)

    def _fallback_toc(self, mode):
        if mode == "CUSTOMS":
            return ["1. ç”³æŠ¥è¦ç´ å¤æ ¸", "2. ä»·æ ¼é€»è¾‘åˆ†æ", "3. ç›‘ç®¡è¯ä»¶ç­›æŸ¥", "4. ç»“è®ºä¸å»ºè®®"]
        return ["1. èƒŒæ™¯æ¦‚è¿°", "2. æ ¸å¿ƒäº‹å®æ¢³ç†", "3. æ·±åº¦å…³è”åˆ†æ", "4. ç»“è®ºä¸å±•æœ›"]

    def _sse(self, type_str, payload):
        return f"data: {json.dumps({'type': type_str, 'payload': payload}, ensure_ascii=False)}\n\n"
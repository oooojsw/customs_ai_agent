# RAG检索问题分析报告

**日期**: 2025-01-19
**问题类型**: RAG检索质量问题
**影响范围**: 功能三（合规建议书）
**状态**: 🔍 已分析原因，待修复

---

## 📋 问题描述

### 现象
用户在使用**功能三（合规建议书）**时发现：
1. **不管搜索什么关键词**，都返回同一个文件的同一个chunk
2. **检索到的内容只有一个分号"；"**，看起来是空的
3. **来源文件总是**：`2022年版《进出口税则商品及品目注释》（20250107更新）-页面-3.pdf`

### 用户反馈
> "每次不管rag检索什么词，都是搜索到一个文件的一个chunk"
> "前端不显示，还是一样的原因"

---

## 🔍 问题调查过程

### 调查1: 测试不同查询的检索结果

**测试查询**:
- "基础信息检查"
- "海关征税"
- "禁限货物"
- "价格审查"
- "HS编码"
- "进出口税则"
- "商品归类"

**测试结果**:
```
查询: 基础信息检查
  来源: 2022年版《进出口税则商品及品目注释》（20250107更新）-页面-3.pdf
  相似度: 55.59%
  内容Hash: -1905383043867788060
  内容: 133字符（正常）

查询: 海关征税
  来源: 2022年版《进出口税则商品及品目注释》（20250107更新）-页面-3.pdf
  相似度: 54.33%
  内容Hash: 238013458835562381
  内容: ；（1字符）
```

**关键发现**:
- ✅ **7个查询中有6个返回同一个chunk**（hash相同）
- ✅ **所有查询都返回同一个文件**
- ❌ **大部分查询返回的只有分号"；"**

---

### 调查2: FAISS索引Chunk大小分布

**统计数据**:
```
总chunk数: 15,970
├─ large (>500字符): 13,354 个 (83.6%) ✅ 正常
├─ medium (100-500): 2,118 个 (13.3%) ✅ 正常
├─ small (10-100): 12 个 (0.1%) ⚠️ 边界
└─ tiny (≤10字符): 486 个 (3.1%) ❌ 问题！
```

**486个极小chunk的示例**:
```
文件: 2022年版《进出口税则商品及品目注释》（20250107更新）-页面-2.pdf
大小: 1 字符
内容: '。' 或 '；'
```

**来源分布**:
```
2022年版《进出口税则商品及品目注释》（20250107更新）.pdf: 188 个
2022年版《进出口税则商品及品目注释》（20250107更新）-页面-2.pdf: 108 个
2022年版《进出口税则商品及品目注释》（20250107更新）-页面-3.pdf: 80 个
中华人民共和国进出口税则（2025）.pdf: 34 个
...（8个PDF文件都有这个问题）
```

---

### 调查3: 问题Chunk的Embedding分析

**测试搜索"；"**:
```python
查询: "；"
Top 3结果:
  1. 2022年版《进出口税则商品及品目注释》（20250107更新）-页面-3.pdf (；) - 100.00%
  2. 2022年版《进出口税则商品及品目注释》（20250107更新）.pdf (；) - 100.00%
  3. 《中华人民共和国海关进出口商品涉税规范申报目录》（2025年版）.pdf (；) - 100.00%
```

**关键发现**:
- ✅ 所有只有分号的chunk的**embedding几乎完全相同**
- ✅ 这些chunk在向量空间中形成了**"聚类"**
- ✅ 搜索";"时，相似度**100%**

---

### 调查4: 查询Embedding的相似度分析

**测试查询之间的相似度**:
```
'基础信息检查' 与 '；': 59.85%
'海关征税' 与 '；': 58.29%
'禁限货物' 与 '；': 67.51%
'价格审查' 与 '；': 67.51%
'HS编码' 与 '；': 75.08% ⚠️ 最高！
'进出口税则' 与 '；': 59.24%
```

**异常发现**:
```
'禁限货物' 和 '价格审查' 的相似度: 100.00% ⚠️
```

---

## 🎯 问题根源分析

### 根本原因：三个核心问题

#### 问题1: Embedding模型不合适 ❌

**模型**: **all-MiniLM-L6-v2**
**类型**: 英文模型
**问题**: 对中文语义理解不够好

**具体表现**:
- 中文句子之间的embedding相似度普遍偏高（50-70%）
- 标点符号"；"的embedding位于向量空间的"中心"区域
- 导致"；"与各种中文查询都有较高的相似度（58-75%）

**证据**:
```
查询 "HS编码" 与 "；" 的相似度: 75.08%
查询 "测试" 与 "；" 的相似度: 90.21%
```

---

#### 问题2: 大量无意义的Chunk被索引 ❌

**数量**: 484个（3.1%）
**内容**: 只有"；"、"。"等标点符号
**来源**: PDF切分时的边界问题

**为什么会产生**:
```python
# 切分器配置
RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=150,
    separators=[
        "。\n", "！\n", "？\n", "；\n",
        "\n\n\n", "\n\n", "\n",
        "。", "！", "？", "；",  # ← 这里
        "，", " ", ""
    ]
)
```

当PDF内容在页面结尾、章节分隔等位置时：
- 切分器恰好在这些分隔符处切分
- 产生了只有"；"的chunk
- 这些chunk也被向量化并加入FAISS索引

---

#### 问题3: 没有质量过滤机制 ❌

**检索逻辑**:
```python
# 当前后端检索代码
results = await kb.search_with_score(query, k=3)
if results:
    doc, similarity = results[0]
    snippet = doc.page_content  # ← 直接使用，不检查长度
    # 发送给前端
```

**问题**:
- 没有过滤掉长度<50的chunk
- 没有检查chunk的质量
- 导致"；"这种无意义chunk被返回给前端

---

## 📊 问题链路图

```
┌─────────────────────────────────────────────────────────────┐
│                     问题根源                               │
└─────────────────────────────────────────────────────────────┘
                              │
          ┌─────────────────┼─────────────────┐
          │                 │                 │
          ▼                 ▼                 ▼
    ┌──────────┐      ┌──────────┐      ┌──────────┐
    │问题1:    │      │问题2:    │      │问题3:    │
    │Embedding │      │Chunk质量 │      │无过滤   │
    │模型不适  │      │486个    │      │机制     │
    │合中文    │      │无用chunk │      │          │
    └─────┬────┘      └─────┬────┘      └─────┬────┘
          │                │                │
          └─────────────────┼─────────────────┘
                            ▼
              ┌──────────────────────┐
              │  Chunk被向量化      │
              │  (包括";"chunk)    │
              └──────────┬───────────┘
                         ▼
              ┌──────────────────────┐
              │  加入FAISS索引       │
              │  15,970个chunk      │
              └──────────┬───────────┘
                         ▼
              ┌──────────────────────┐
              │  用户查询            │
              │  "海关征税"          │
              └──────────┬───────────┘
                         ▼
              ┌──────────────────────┐
              │  Embedding相似度     │
              │  查询与";": 58-75%  │
              └──────────┬───────────┘
                         ▼
              ┌──────────────────────┐
              │  FAISS检索           │
              │  Top K = 3          │
              └──────────┬───────────┘
                         ▼
              ┌──────────────────────┐
              │  返回";" chunk      │
              │  (486个之一)        │
              └──────────┬───────────┘
                         ▼
              ┌──────────────────────┐
              │  前端显示            │
              │  只有一个"；"      │
              │  看起来是空的        │
              └──────────────────────┘
```

---

## 📈 数据统计汇总

### FAISS索引状态
```
总chunk数: 15,970
文档来源: 15个（13个PDF + 2个TXT）
向量维度: 384
Embedding模型: all-MiniLM-L6-v2
```

### Chunk质量分布
```
高质量chunk (>100字符): 13,354 (83.6%) ✅
中等质量 (10-100): 2,118 (13.3%) ✅
低质量 (≤10字符): 486 (3.1%) ❌ 问题
```

### 有问题Chunk的来源
```
2022年版《进出口税则商品及品目注释》（20250107更新）.pdf: 188 个
2022年版《进出口税则商品及品目注释》（20250107更新）-页面-2.pdf: 108 个
2022年版《进出口税则商品及品目注释》（20250107更新）-页面-3.pdf: 80 个
中华人民共和国进出口税则（2025）.pdf: 34 个
《中华人民共和国海关进出口商品涉税规范申报目录》（2025年版）.pdf: 20 个
进出口商品涉税规范申报目录.pdf: 20 个
中华人民共和国进出口税则（2025）-页面-1.pdf: 18 个
中华人民共和国进出口税则（2025）-页面-2.pdf: 16 个
```

**总计**: 484个无用chunk，来自8个PDF文件

---

## 💡 建议的解决方案（待实施）

### 方案1: 切分后过滤小Chunk（推荐）⭐⭐⭐⭐⭐

**位置**: `src/services/knowledge_base.py`
**优先级**: 高
**难度**: 低

```python
# 在 _create_index() 中添加过滤
chunks = text_splitter.split_documents(all_docs)

# 🔥 新增：过滤掉小于50字符的chunk
chunks = [c for c in chunks if len(c.page_content) >= 50]

print(f"📄 过滤前: {len(all_docs)} 个文档")
print(f"📄 过滤后: {len(chunks)} 个chunk")
```

**预期效果**:
- 减少3.1%的无用chunk（486个）
- 提升检索质量
- 不影响正常内容（99%的chunk保留）

---

### 方案2: 检索时过滤（临时方案）⭐⭐⭐⭐

**位置**: `src/services/report_agent.py`
**优先级**: 中
**难度**: 低

```python
# 在检索后添加过滤
results = await kb.search_with_score(query, k=3)

# 🔥 新增：过滤掉小于50字符的结果
filtered_results = [
    (doc, score) for doc, score in results
    if len(doc.page_content) >= 50
]

if filtered_results:
    doc, similarity = filtered_results[0]
    # ...
```

**优点**:
- 快速修复
- 不需要重建索引

**缺点**:
- 每次检索都要过滤
- 性能略有影响

---

### 方案3: 更换中文Embedding模型（长期方案）⭐⭐⭐⭐⭐

**当前**: `all-MiniLM-L6-v2` (英文)
**建议**: `m3e-base` 或 `bge-large-zh` (中文)
**优先级**: 高
**难度**: 中

**推荐模型**:
1. **m3e-base** (Moka AI)
   - 专门针对中文优化
   - 在中文检索任务上表现优异
   - 模型大小: 102MB

2. **bge-large-zh** (BAAI)
   - 北京智源研究院
   - 中文语义理解能力强
   - 模型大小: 1.34GB

**实施步骤**:
```python
# requirements.txt 添加
m3e-base>=1.0.0

# knowledge_base.py 修改
from langchain_community.embeddings import HuggingFaceEmbeddings

self.embeddings = HuggingFaceEmbeddings(
    model_name="m3e-base",  # ← 中文模型
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)
```

**预期效果**:
- 中文查询的embedding更准确
- 标点符号的embedding会更"边缘化"
- 检索质量显著提升

---

### 方案4: 优化PDF切分策略（辅助方案）⭐⭐⭐

**问题**: 切分器在页面边界产生小chunk
**解决**: 调整分隔符优先级

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=150,
    separators=[
        "。\n", "！\n", "？\n", "；\n",
        "\n\n\n", "\n\n", "\n",
        "。", "！", "？", "；",  # ← 这些放在后面
        "，", " ", ""  # ← 最后才按字符切分
    ]
)

# 🔥 改进：先合并小chunk
# 在切分后，将相邻的小chunk合并
```

---

## 📝 修复优先级建议

### 立即修复（今天）
✅ **方案1**: 切分后过滤小Chunk
- 位置: `knowledge_base.py`
- 工作量: 5行代码
- 效果: 立竿见影

### 短期优化（本周）
✅ **方案3**: 更换中文Embedding模型
- 位置: `knowledge_base.py`
- 工作量: 安装模型 + 修改配置
- 效果: 根本性提升

### 长期优化（下周）
✅ **方案4**: 优化PDF切分策略
- 位置: PDF预处理
- 工作量: 需要测试不同策略
- 效果: 进一步提升chunk质量

---

## 🔬 测试数据记录

### 测试脚本
- `test_different_queries.py` - 测试不同查询的检索结果
- `analyze_problem_chunk.py` - 分析问题chunk
- `check_embedding_similarity.py` - 检查embedding相似度
- `check_all_chunks.py` - 检查chunk大小分布

### 测试结果
所有测试脚本均已保存，可复现问题。

---

## 📊 影响评估

### 用户体验影响
- ⭐⭐⭐⭐⭐ **严重**: RAG检索返回无效内容
- 用户无法看到真实的PDF法规内容
- 功能三（合规建议书）不可用

### 系统性能影响
- ⭐⭐ **轻微**: 3%的chunk是无用的
- 索引大小浪费约 3%
- 检索速度正常

### 数据完整性
- ✅ PDF内容已正确提取（775万字符）
- ✅ 数据库缓存正常工作
- ❌ 只是检索质量有问题

---

## 📅 修复计划

### 第1步: 快速修复（预计30分钟）
- 实施方案1：过滤小chunk
- 重建FAISS索引
- 测试验证

### 第2步: 根本优化（预计2小时）
- 实施方案3：更换中文embedding模型
- 重建FAISS索引
- 全面测试

### 第3步: 质量保证（预计1小时）
- 回归测试
- 性能测试
- 用户验收

**总计**: 3.5小时

---

## 📌 问题状态

| 状态 | 说明 |
|------|------|
| 🔍 **问题分析** | ✅ 已完成 |
| 🎯 **原因确认** | ✅ 已完成 |
| 💡 **方案设计** | ✅ 已完成 |
| 🔧 **待实施** | ⏳ 等待批准 |
| ✅ **测试验证** | ⏳ 修复后进行 |

---

## 📞 联系方式

如有疑问，请查阅：
- 测试脚本：`test_*.py`
- 分析脚本：`analyze_*.py`, `check_*.py`
- PDF实施总结：`PDF_IMPLEMENTATION_SUMMARY.md`

---

**报告人**: Claude
**报告日期**: 2025-01-19
**文档版本**: v1.0
**状态**: 🔍 分析完成，待修复

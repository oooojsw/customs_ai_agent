import json
import asyncio
import httpx
from typing import List, AsyncGenerator
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from pathlib import Path

# å¯¼å…¥é…ç½®
from src.config.loader import settings

class ComplianceReporter:
    def __init__(self):
        print("ğŸ“‘ [System] åˆå§‹åŒ–åˆè§„æŠ¥å‘Šç”Ÿæˆå¼•æ“ (DeepSeek Powered)...")
        
        # 1. ç½‘ç»œå±‚é…ç½® (ä¿®å¤ç‚¹ï¼šåŒºåˆ† Sync å’Œ Async Transport)
        proxy_url = settings.HTTP_PROXY
        
        # âŒ é”™è¯¯ä»£ç  (åŸ): transport = httpx.HTTPTransport(...)
        # âœ… ä¿®æ­£ä»£ç  (æ–°): ä½¿ç”¨ AsyncHTTPTransport
        async_transport = httpx.AsyncHTTPTransport(proxy=proxy_url, verify=False)
        
        # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
        self.async_client = httpx.AsyncClient(transport=async_transport, timeout=120.0)

        # 2. LLM åˆå§‹åŒ– (DeepSeek)
        self.llm = ChatOpenAI(
            model=settings.DEEPSEEK_MODEL,
            api_key=settings.DEEPSEEK_API_KEY,
            base_url=settings.DEEPSEEK_BASE_URL,
            temperature=0.3, # æŠ¥å‘Šéœ€è¦ç›¸å¯¹ä¸¥è°¨
            # å…³é”®ï¼šæ³¨å…¥ä¿®å¤åçš„å¼‚æ­¥å®¢æˆ·ç«¯
            http_async_client=self.async_client,
            streaming=True
        )

        # 3. åŠ è½½ SOP
        self.sop_content = self._load_sop()

    def _load_sop(self) -> str:
        """åŠ è½½ç³»ç»Ÿçº§ SOP æ–‡ä»¶"""
        try:
            # è·å– config/sop_process.txt çš„ç»å¯¹è·¯å¾„
            base_dir = Path(__file__).resolve().parent.parent.parent
            sop_path = base_dir / "config" / "sop_process.txt"
            
            if sop_path.exists():
                with open(sop_path, "r", encoding="utf-8") as f:
                    return f.read()
            return "æ— ç³»ç»ŸSOPæ–‡ä»¶ï¼Œè¯·æ ¹æ®é€šç”¨æµ·å…³æ³•è§„è¿›è¡Œåˆ†æã€‚"
        except Exception as e:
            print(f"âš ï¸ SOP åŠ è½½å¤±è´¥: {e}")
            return "æ— ç³»ç»ŸSOPæ–‡ä»¶ã€‚"

    async def generate_stream(self, raw_data: str) -> AsyncGenerator[str, None]:
        """
        æ ¸å¿ƒç”Ÿæˆæµï¼šè§„åˆ’ -> æ‰§è¡Œå¾ªç¯ -> å®Œç»“
        yield: SSE æ ¼å¼å­—ç¬¦ä¸²
        """
        try:
            # --- é˜¶æ®µ 1: è§„åˆ’ (Planning) ---
            yield self._sse_pack("planning", "æ­£åœ¨åˆ†ææ•°æ®å¹¶æ ¹æ® SOP è§„åˆ’æŠ¥å‘Šç›®å½•...")
            
            # è°ƒç”¨ LLM ç”Ÿæˆç›®å½•
            toc_list = await self._generate_toc(raw_data)
            
            # æ¨é€ç›®å½•ç»™å‰ç«¯æ¸²æŸ“
            yield self._sse_pack("toc_generated", {"steps": toc_list})
            await asyncio.sleep(0.5) 

            # --- é˜¶æ®µ 2: æ‰§è¡Œ (Executing) ---
            # ç»´æŠ¤å¯¹è¯å†å² (Context Caching)
            history_messages = [
                SystemMessage(content=f"""ä½ æ˜¯ä¸€åèµ„æ·±æµ·å…³åˆè§„å®¡è®¡ä¸“å®¶ã€‚
ä¾æ®ä»¥ä¸‹ SOP æ ‡å‡†æµç¨‹ï¼š
{self.sop_content}

ä½ éœ€è¦æ’°å†™ä¸€ä»½ä¸“ä¸šçš„ã€Šè¿›å‡ºå£è´§ç‰©åˆè§„æ€§å®¡æŸ¥æŠ¥å‘Šã€‹ã€‚
è¯·ä¿æŒè¯­æ°”å®¢è§‚ã€ä¸“ä¸šï¼Œé‡ç‚¹æŒ‡å‡ºé£é™©ç‚¹ã€‚ä¸è¦ä½¿ç”¨ Markdown ä»£ç å—åŒ…è£¹æ•´ä¸ªå›å¤ã€‚"""),
                HumanMessage(content=f"è¿™æ˜¯å¾…å®¡æŸ¥çš„æŠ¥å…³æ•°æ®ï¼š\n{raw_data}\n\nè¯·æŒ‰ç…§è®¡åˆ’æ’°å†™æŠ¥å‘Šã€‚")
            ]

            # å¾ªç¯ç”Ÿæˆæ¯ä¸€ç« 
            for index, section_title in enumerate(toc_list):
                # 2.1 æ¨é€çŠ¶æ€ï¼šå¼€å§‹å†™è¿™ä¸€ç« 
                yield self._sse_pack("step_start", {"index": index, "title": section_title})
                
                # 2.2 æ„å»ºå½“å‰ç« èŠ‚çš„ Prompt
                step_prompt = f"è¯·æ’°å†™æŠ¥å‘Šçš„ç¬¬ {index + 1} éƒ¨åˆ†ï¼šã€{section_title}ã€‘ã€‚\nè¦æ±‚ï¼šå†…å®¹è¯¦å®ï¼Œå¦‚æœå¼•ç”¨äº†æ³•è§„è¯·æ³¨æ˜ã€‚ç›´æ¥è¾“å‡º Markdown å†…å®¹ï¼Œä¸è¦åŒ…å«'å¥½çš„'ã€'ä¸‹é¢æ˜¯...'ç­‰åºŸè¯ã€‚"
                
                # ä¸´æ—¶æ¶ˆæ¯åˆ—è¡¨
                current_turn_messages = history_messages + [HumanMessage(content=step_prompt)]
                
                section_content = ""
                
                # 2.3 æµå¼ç”Ÿæˆå†…å®¹
                async for chunk in self.llm.astream(current_turn_messages):
                    if chunk.content:
                        content = chunk.content
                        section_content += content
                        yield self._sse_pack("step_stream", {"chunk": content})
                
                # 2.4 ä¸Šä¸‹æ–‡ç¼“å­˜ï¼šå°†ç»“æœå­˜å…¥å†å²
                history_messages.append(HumanMessage(content=step_prompt))
                history_messages.append(AIMessage(content=section_content))
                
                # 2.5 æ¨é€çŠ¶æ€ï¼šè¿™ä¸€ç« å®Œæˆ
                yield self._sse_pack("step_done", {"index": index})
                
                await asyncio.sleep(0.5)

            # --- é˜¶æ®µ 3: å®Œç»“ ---
            yield self._sse_pack("done", {})
            
        except Exception as e:
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆæµä¸­æ–­: {e}")
            # å‘é€é”™è¯¯ç»™å‰ç«¯
            yield f"data: {json.dumps({'type': 'error', 'payload': str(e)}, ensure_ascii=False)}\n\n"

    async def _generate_toc(self, raw_data: str) -> List[str]:
        """ä½¿ç”¨ LLM ç”Ÿæˆç›®å½•ç»“æ„ (JSON)"""
        prompt = f"""
åˆ†æä»¥ä¸‹æŠ¥å…³æ•°æ®å’Œ SOPï¼Œåˆ—å‡ºåˆè§„å®¡æŸ¥æŠ¥å‘Šçš„ç« èŠ‚ç›®å½•ã€‚
SOPæ‘˜è¦: {self.sop_content[:200]}...
æ•°æ®æ‘˜è¦: {raw_data[:200]}...

è¦æ±‚ï¼š
1. åªè¿”å›ä¸€ä¸ª JSON å­—ç¬¦ä¸²æ•°ç»„ã€‚
2. åŒ…å« 3-5 ä¸ªæ ¸å¿ƒç« èŠ‚æ ‡é¢˜ã€‚
3. å¿…é¡»åŒ…å«â€œé£é™©åˆ†æâ€å’Œâ€œæ”¹è¿›å»ºè®®â€ç›¸å…³çš„ç« èŠ‚ã€‚
4. ä¸¥ç¦è¾“å‡º Markdown ä»£ç å—æ ‡è®°ï¼Œåªè¾“å‡ºçº¯æ•°ç»„å­—ç¬¦ä¸²ã€‚
"""
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªJSONç”Ÿæˆå™¨ã€‚åªè¾“å‡ºJSONæ•°ç»„ï¼Œä¸è¦ä»»ä½•å…¶ä»–åºŸè¯ã€‚"),
            HumanMessage(content=prompt)
        ]
        
        try:
            response = await self.llm.ainvoke(messages)
            content = response.content.strip()
            content = content.replace("```json", "").replace("```", "").strip()
            
            toc = json.loads(content)
            if isinstance(toc, list):
                return toc
            return ["1. ç»¼åˆåˆ†æ", "2. é£é™©æç¤º", "3. æ”¹è¿›å»ºè®®"]
        except Exception as e:
            print(f"âŒ ç›®å½•ç”Ÿæˆå¤±è´¥: {e}")
            return ["1. æ•°æ®æ¦‚è§ˆ", "2. è¯¦ç»†å®¡æŸ¥", "3. æ€»ç»“"]

    def _sse_pack(self, event_type: str, data: any) -> str:
        return f"data: {json.dumps({'type': event_type, 'payload': data}, ensure_ascii=False)}\n\n"
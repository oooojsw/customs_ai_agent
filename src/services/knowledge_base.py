import os
import shutil
from pathlib import Path
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

class KnowledgeBase:
    def __init__(self):
        # 1. å®šä¹‰ç»å¯¹è·¯å¾„
        # __file__ æ˜¯å½“å‰è„šæœ¬æ–‡ä»¶çš„è·¯å¾„
        # .parent.parent.parent å›é€€ä¸‰å±‚æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
        self.base_dir = Path(__file__).resolve().parent.parent.parent
        self.data_path = self.base_dir / "data" / "knowledge"
        
        # å‘é‡æ•°æ®åº“æœ€ç»ˆä¿å­˜ç›®å½•
        self.vector_db_path = self.base_dir / "config" / "faiss_index_local"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.vector_db_path.mkdir(parents=True, exist_ok=True)

        print(f"âš™ï¸ [KnowledgeBase] åˆå§‹åŒ–æœ¬åœ° Embedding æ¨¡å‹ (all-MiniLM-L6-v2)...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # åŠ è½½æˆ–é‡å»ºç´¢å¼•
        self.vector_store = self._load_or_create_index()

    def _load_or_create_index(self):
        # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        index_file = self.vector_db_path / "index.faiss"
        
        if index_file.exists():
            print("ğŸ“‚ [KnowledgeBase] åŠ è½½æœ¬åœ°å‘é‡ç´¢å¼• (Hit Cache)...")
            try:
                return FAISS.load_local(
                    str(self.vector_db_path), 
                    self.embeddings,
                    allow_dangerous_deserialization=True 
                )
            except Exception as e:
                print(f"âš ï¸ ç´¢å¼•æ–‡ä»¶æŸåï¼Œæ­£åœ¨é‡å»º: {e}")
                return self._create_index()
        else:
            print("âš™ï¸ [KnowledgeBase] æœ¬åœ°æ— ç´¢å¼•ï¼Œæ­£åœ¨é‡å»ºå‘é‡æ•°æ®åº“...")
            return self._create_index()

    def _create_index(self):
        if not self.data_path.exists():
            print(f"âš ï¸ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_path}ï¼Œå°†åˆ›å»ºç©ºç´¢å¼•ã€‚")
            return FAISS.from_texts(["åˆå§‹åŒ–ç©ºç™½æ–‡æ¡£"], self.embeddings)

        # 1. åŠ è½½æ–‡æ¡£
        loaders = [
            DirectoryLoader(str(self.data_path), glob="**/*.txt", loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"}),
            DirectoryLoader(str(self.data_path), glob="**/*.md", loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"}),
        ]
        
        documents = []
        for loader in loaders:
            try:
                documents.extend(loader.load())
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ–‡ä»¶å‡ºé”™: {e}")

        if not documents:
            print("âš ï¸ æœªæ‰¾åˆ°æ–‡æ¡£ï¼Œåˆ›å»ºç©ºç´¢å¼•ã€‚")
            return FAISS.from_texts(["æ— æ•°æ®"], self.embeddings)

        # 2. åˆ‡åˆ†æ–‡æ¡£
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = text_splitter.split_documents(documents)
        print(f"ğŸ“„ æ­£åœ¨å‘é‡åŒ– {len(chunks)} ä¸ªæ–‡æœ¬ç‰‡æ®µ...")

        # 3. åˆ›å»ºå‘é‡åº“ (å†…å­˜ä¸­)
        vector_store = FAISS.from_documents(chunks, self.embeddings)

        # 4. ä¿å­˜åˆ°æœ¬åœ° (ã€â­ æ ¸å¿ƒä¿®å¤ï¼šå…ˆå­˜ä¸´æ—¶ç›®å½•ï¼Œå†æ¬è¿ã€‘)
        try:
            # å®šä¹‰ä¸€ä¸ªçº¯è‹±æ–‡ã€æ— ç©ºæ ¼çš„ä¸´æ—¶ç›®å½•å
            temp_dir_name = "temp_faiss_build"
            temp_path = self.base_dir / temp_dir_name
            
            # å¦‚æœä¸Šæ¬¡å¼‚å¸¸é€€å‡ºæ®‹ç•™äº†ä¸´æ—¶ç›®å½•ï¼Œå…ˆåˆ æ‰
            if temp_path.exists():
                shutil.rmtree(temp_path)

            # A. ä¿å­˜åˆ°ä¸´æ—¶ç›®å½• (FAISS å¯¹è¿™é‡Œçš„è·¯å¾„å¾ˆæ»¡æ„)
            # æ³¨æ„ï¼šsave_local æ¥å—çš„æ˜¯æ–‡ä»¶å¤¹è·¯å¾„å­—ç¬¦ä¸²
            vector_store.save_local(temp_dir_name)

            # B. æ¬è¿æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½• (Python å¤„ç†ä¸­æ–‡è·¯å¾„å¾ˆå¼º)
            # å…ˆæ¸…ç©ºç›®æ ‡ç›®å½•
            if self.vector_db_path.exists():
                shutil.rmtree(self.vector_db_path)
            self.vector_db_path.mkdir(parents=True, exist_ok=True)

            # ç§»åŠ¨æ–‡ä»¶ (index.faiss å’Œ index.pkl)
            for file_name in os.listdir(temp_dir_name):
                src_file = temp_path / file_name
                dst_file = self.vector_db_path / file_name
                shutil.move(str(src_file), str(dst_file))

            # C. åˆ é™¤ä¸´æ—¶ç›®å½•
            shutil.rmtree(temp_path)

            print(f"ğŸ’¾ ç´¢å¼•å·²æˆåŠŸæ„å»ºå¹¶ä¿å­˜è‡³: {self.vector_db_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç´¢å¼•å¤±è´¥ (ä¸å½±å“æœ¬æ¬¡è¿è¡Œï¼Œä½†ä¸‹æ¬¡éœ€é‡å»º): {e}")
        
        return vector_store

    def get_retriever(self):
        return self.vector_store.as_retriever(search_kwargs={"k": 3})

    async def search_with_score(self, query: str, k: int = 6):
        """
        æ‰§è¡Œå‘é‡æ£€ç´¢å¹¶è¿”å›çœŸå®ç›¸ä¼¼åº¦åˆ†æ•°

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›ç»“æœæ•°é‡

        Returns:
            List[Tuple[Document, float]]: æ–‡æ¡£å’Œå¯¹åº”çš„ç›¸ä¼¼åº¦åˆ†æ•°
            æ³¨æ„ï¼šFAISS è¿”å›çš„æ˜¯è·ç¦»ï¼ˆL2è·ç¦»ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºç›¸ä¼¼åº¦ç™¾åˆ†æ¯”
        """
        # similarity_search_with_score è¿”å› (Document, score)
        # score æ˜¯ L2 è·ç¦»çš„å¹³æ–¹ï¼Œè¶Šå°è¶Šç›¸ä¼¼ï¼ˆ0 è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼‰
        results = self.vector_store.similarity_search_with_score(query, k=k)

        # å°† L2 è·ç¦»çš„å¹³æ–¹è½¬æ¢ä¸ºç›¸ä¼¼åº¦ç™¾åˆ†æ¯”
        # FAISS è¿”å›çš„æ˜¯ squared L2 è·ç¦»ï¼Œå¯¹äºå½’ä¸€åŒ–å‘é‡èŒƒå›´æ˜¯ [0, 4]
        # ç›¸ä¼¼åº¦ = (1 - sqrt(distance)/2)
        import math
        processed_results = []
        for doc, squared_distance in results:
            # å–å¹³æ–¹æ ¹å¾—åˆ°çœŸå®çš„ L2 è·ç¦»
            distance = math.sqrt(max(0, float(squared_distance)))
            # å¯¹äºå½’ä¸€åŒ–å‘é‡ï¼ŒL2 è·ç¦»èŒƒå›´æ˜¯ [0, 2]
            distance = min(distance, 2.0)
            # è½¬æ¢ä¸ºç›¸ä¼¼åº¦ (0-1èŒƒå›´)ï¼Œå‰ç«¯æ˜¾ç¤ºæ—¶ä¹˜100
            similarity = float((1 - distance / 2))
            processed_results.append((doc, similarity))

        return processed_results
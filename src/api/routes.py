import traceback
from fastapi import APIRouter, HTTPException, Request, UploadFile, File, Depends
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional

# --- æ ¸å¿ƒæœåŠ¡å¯¼å…¥ ---
from src.services.data_client import DataClient
from src.core.orchestrator import RiskAnalysisOrchestrator
from src.services.report_agent import ComplianceReporter
from src.database.pdf_repository import PDFRepository

# å®¹é”™å¯¼å…¥
try:
    from src.services.image_extractor import ImageTextExtractor, NotDeclarationError
except ImportError:
    ImageTextExtractor = None

# --- æ‰¹é‡å¤„ç†ä¸æ•°æ®åº“ (ä¿ç•™å…¨é‡åŠŸèƒ½) ---
try:
    from src.services.batch_processor import BatchProcessor, start_batch_processing
    from src.database.connection import AsyncSessionLocal
    from src.database.crud import BatchRepository
    BATCH_AVAILABLE = True
except ImportError:
    BATCH_AVAILABLE = False
    print("âš ï¸ [System] æ•°æ®åº“ç›¸å…³ä¾èµ–æœªå®Œå…¨å®‰è£…ï¼Œæ‰¹é‡åŠŸèƒ½å°†å—é™")

router = APIRouter()

# --- è¾…åŠ©å‡½æ•°ï¼šåŠ¨æ€è·å– LLM é…ç½® ---
async def get_current_llm_config(req: Request) -> dict:
    """
    åŠ¨æ€è·å–å½“å‰ LLM é…ç½®ï¼Œæ¯æ¬¡éƒ½æ£€æŸ¥æ•°æ®åº“çš„ is_enabled çŠ¶æ€

    Returns:
        é…ç½®å­—å…¸ {
            'api_key': str,
            'base_url': str,
            'model': str,
            'temperature': float,
            'source': 'user' | 'env'
        }
    """
    try:
        from src.config.llm_loader import llm_config_loader

        # æ¯æ¬¡éƒ½ä»æ•°æ®åº“é‡æ–°åŠ è½½é…ç½®ï¼Œæ£€æŸ¥ is_enabled çŠ¶æ€
        async with AsyncSessionLocal() as db:
            config = await llm_config_loader.load_config(db)
        return config

    except Exception as e:
        print(f"[Config] é…ç½®è·å–å¤±è´¥: {e}ï¼Œå›é€€åˆ° .env")
        from src.config.loader import settings
        return {
            'api_key': settings.DEEPSEEK_API_KEY,
            'base_url': settings.DEEPSEEK_BASE_URL,
            'model': settings.DEEPSEEK_MODEL,
            'temperature': 0.3,
            'source': 'env'
        }

# --- è¯·æ±‚ä½“å®šä¹‰ ---
class AnalysisRequest(BaseModel):
    raw_data: str
    language: str = "zh"  # æ–°å¢ï¼šè¯­è¨€å‚æ•°ï¼Œé»˜è®¤ä¸­æ–‡

class ChatRequest(BaseModel):
    message: str
    session_id: str = "default_session"
    language: str = "zh"  # æ–°å¢ï¼šè¯­è¨€å‚æ•°ï¼Œé»˜è®¤ä¸­æ–‡

class ReportRequest(BaseModel):
    raw_data: str
    language: str = "zh"  # æ–°å¢ï¼šè¯­è¨€å‚æ•°ï¼Œé»˜è®¤ä¸­æ–‡

# ==========================================
# 1. æ™ºèƒ½å®¡å•æ¥å£ (åŠŸèƒ½ä¸€)
# ==========================================
@router.post("/analyze")
async def analyze_customs_declaration(request: AnalysisRequest, req: Request):
    if not request.raw_data or len(request.raw_data.strip()) < 5:
        raise HTTPException(status_code=400, detail="æ•°æ®å¤ªçŸ­ï¼Œæ— æ³•åˆ†æ")

    # åŠ¨æ€è·å– LLM é…ç½®ï¼ˆæ¯æ¬¡éƒ½æ£€æŸ¥æ•°æ®åº“çš„ is_enabled çŠ¶æ€ï¼‰
    llm_config = await get_current_llm_config(req)
    print(f"[åŠŸèƒ½ä¸€] ä½¿ç”¨é…ç½®æ¥æº: {llm_config['source']}")

    orchestrator = RiskAnalysisOrchestrator(llm_config=llm_config)
    return StreamingResponse(
        orchestrator.analyze_stream(request.raw_data, language=request.language),
        media_type="text/event-stream"
    )

# ==========================================
# 2. æ³•è§„å’¨è¯¢æ¥å£ (åŠŸèƒ½äºŒ)
# ==========================================
@router.post("/chat")
async def chat_with_agent(body: ChatRequest, request: Request):
    # åŠ¨æ€è·å–é…ç½®å¹¶åˆ›å»ºä¸´æ—¶ agent
    llm_config = await get_current_llm_config(request)

    # è·å–å…¨å±€ kb å®ä¾‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    kb = getattr(request.app.state, "kb", None)

    # ä½¿ç”¨å½“å‰é…ç½®åˆ›å»ºä¸´æ—¶ agent
    from src.services.chat_agent import CustomsChatAgent
    agent = CustomsChatAgent(kb=kb, llm_config=llm_config)

    return StreamingResponse(
        agent.chat_stream(body.message, body.session_id, language=body.language),
        media_type="text/event-stream"
    )

# ==========================================
# 3. æŠ¥å‘Šç”Ÿæˆæ¥å£ (åŠŸèƒ½ä¸‰)
# ==========================================
@router.post("/generate_report")
async def generate_compliance_report(body: ReportRequest, req: Request):
    try:
        # åŠ¨æ€è·å–é…ç½®å¹¶åˆ›å»ºä¸´æ—¶ reporter
        llm_config = await get_current_llm_config(req)

        # è·å–å…¨å±€ kb å®ä¾‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        kb = getattr(req.app.state, "kb", None)

        # ä½¿ç”¨å½“å‰é…ç½®åˆ›å»ºä¸´æ—¶ reporter
        reporter = ComplianceReporter(kb=kb, llm_config=llm_config)

        return StreamingResponse(
            reporter.generate_stream(body.raw_data, language=body.language),
            media_type="text/event-stream"
        )
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æŠ¥å‘Šå¼•æ“å´©æºƒ: {str(e)}")

# ==========================================
# 4. å›¾ç‰‡ OCR è¯†åˆ«
# ==========================================
@router.post("/analyze_image")
async def analyze_declaration_image(
    file: UploadFile = File(...),
    language: str = "zh"  # æ–°å¢ï¼šè¯­è¨€å‚æ•°ï¼Œé»˜è®¤ä¸­æ–‡
):
    if not ImageTextExtractor:
        raise HTTPException(status_code=501, detail="OCR æ¨¡å—ç¼ºå¤±")

    content = await file.read()

    # ä½¿ç”¨å¼‚æ­¥å·¥å‚æ–¹æ³•åˆ›å»ºå®ä¾‹ï¼ˆä»æ•°æ®åº“åŠ è½½é…ç½®ï¼‰
    try:
        from src.database.connection import AsyncSessionLocal
        async with AsyncSessionLocal() as db:
            extractor = await ImageTextExtractor.create_async(db)
    except Exception as e:
        print(f"[Warning] æ•°æ®åº“é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ .env: {e}")
        extractor = ImageTextExtractor()

    try:
        text, model = extractor.extract_text(content, file.content_type, language=language)
        return {"text": text, "model": model}
    except NotDeclarationError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ==========================================
# 5. æ‰¹é‡åˆ†ææ¥å£ (å…¨é‡ä¿ç•™)
# ==========================================
@router.post("/analyze_batch")
async def analyze_batch(file: UploadFile = File(...)):
    if not BATCH_AVAILABLE:
        raise HTTPException(status_code=501, detail="æ•°æ®åº“ä¾èµ–æœªå°±ç»ª")

    if not file.filename.endswith(('.xlsx', '.xls', '.csv')):
        raise HTTPException(status_code=400, detail="æ ¼å¼ä¸æ”¯æŒ")

    content = await file.read()
    try:
        processor = BatchProcessor()
        items = await processor.parse_file(content, file.filename)

        async with AsyncSessionLocal() as db:
            repo = BatchRepository(db)
            task_uuid = await repo.create_batch_task(len(items))
            await repo.add_batch_items(task_uuid, items)

        start_batch_processing(task_uuid)
        return {"status": "success", "task_id": task_uuid, "count": len(items)}
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/analyze_batch/{task_id}")
async def get_batch_progress(task_id: str):
    if not BATCH_AVAILABLE:
        raise HTTPException(status_code=501, detail="æ•°æ®åº“ä¸å¯ç”¨")
    async with AsyncSessionLocal() as db:
        repo = BatchRepository(db)
        result = await repo.get_batch_progress(task_id)
        if not result:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        return {"status": "success", "data": result}

# ==========================================
# 6. å…¶ä»–è¾…åŠ©æ¥å£
# ==========================================
@router.get("/health")
def health_check():
    return {"status": "ok", "version": "3.0.PRO"}

@router.get("/knowledge/content/{filename}")
async def get_knowledge_file_content(filename: str):
    """
    è·å–çŸ¥è¯†åº“æ–‡ä»¶å†…å®¹ï¼ˆç”¨äºå‰ç«¯å±•ç¤º RAG æ£€ç´¢ç»“æœï¼‰

    Args:
        filename: æ–‡ä»¶åï¼ˆå¦‚ "01-1", "test_policy.txt"ï¼‰

    Returns:
        æ–‡ä»¶å†…å®¹æˆ–é”™è¯¯ä¿¡æ¯
    """
    from pathlib import Path
    import os

    try:
        base_dir = Path(__file__).resolve().parent.parent.parent
        knowledge_dir = base_dir / "data" / "knowledge"

        # å°è¯•å¤šä¸ªå¯èƒ½çš„æ–‡ä»¶ååŒ¹é…
        possible_names = [
            filename,
            filename + ".txt",
            filename.replace('.txt', ''),
        ]

        content = None
        matched_file = None

        for name in possible_names:
            file_path = knowledge_dir / name
            if file_path.exists() and file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    matched_file = name
                    break
                except UnicodeDecodeError:
                    # å°è¯•ç”¨å…¶ä»–ç¼–ç 
                    try:
                        with open(file_path, 'r', encoding='gbk') as f:
                            content = f.read()
                        matched_file = name
                        break
                    except:
                        continue

        if content is None:
            # å¦‚æœç›´æ¥åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
            # ä¼˜å…ˆçº§1: å®Œå…¨åŒ¹é…
            # ä¼˜å…ˆçº§2: åŒ…å«åŒ¹é…
            # ä¼˜å…ˆçº§3: ç§»é™¤æ‰©å±•åååŒ¹é…
            found_files = list(knowledge_dir.glob('*'))

            # å…ˆå°è¯•å®Œå…¨åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
            for file_path in found_files:
                if file_path.is_file():
                    file_name = file_path.name
                    if file_name.lower() == filename.lower():
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                            matched_file = file_name
                            break
                        except UnicodeDecodeError:
                            try:
                                with open(file_path, 'r', encoding='gbk') as f:
                                    content = f.read()
                                matched_file = file_name
                                break
                            except:
                                continue

            # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•åŒ…å«åŒ¹é…
            if content is None:
                for file_path in found_files:
                    if file_path.is_file():
                        file_name = file_path.name
                        # è¯·æ±‚çš„æ–‡ä»¶ååŒ…å«åœ¨å®é™…æ–‡ä»¶åä¸­ï¼Œæˆ–å®é™…æ–‡ä»¶ååŒ…å«è¯·æ±‚çš„æ–‡ä»¶å
                        if filename.lower() in file_name.lower() or file_name.lower() in filename.lower():
                            try:
                                with open(file_path, 'r', encoding='utf-8') as f:
                                    content = f.read()
                                matched_file = file_name
                                break
                            except UnicodeDecodeError:
                                try:
                                    with open(file_path, 'r', encoding='gbk') as f:
                                        content = f.read()
                                    matched_file = file_name
                                    break
                                except:
                                    continue

        if content is None:
            # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ .txt æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            available_files = [f.name for f in knowledge_dir.iterdir() if f.is_file() and f.suffix in ['.txt', '.md', '']]
            available_list = "\n".join(sorted(available_files)[:20])  # åªæ˜¾ç¤ºå‰20ä¸ª

            return {
                "status": "not_found",
                "filename": filename,
                "content": f"æœªæ‰¾åˆ°æ–‡ä»¶: {filename}\n\nå¯ç”¨çš„æ–‡ä»¶:\n{available_list}"
            }

        return {
            "status": "success",
            "filename": matched_file,
            "content": content
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {
            "status": "error",
            "filename": filename,
            "content": f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"
        }

@router.get("/query/declaration/{entry_id}")
async def query_declaration_data(entry_id: str):
    client = DataClient()
    text_data = client.fetch_declaration_text(entry_id)
    return {"status": "success", "data": text_data}

# ==========================================
# 7. PDFç®¡ç†æ¥å£ (Markeræ”¯æŒ)
# ==========================================

@router.get("/pdf/stats")
async def get_pdf_stats():
    """
    è·å–PDFç»Ÿè®¡ä¿¡æ¯

    Returns:
        {
            "status": "success",
            "data": {
                "total_documents": int,
                "completed_documents": int,
                "failed_documents": int,
                "total_characters": int,
                "total_processing_time_seconds": float,
                "average_processing_time": float
            }
        }
    """
    try:
        repo = PDFRepository()
        stats = await repo.get_statistics()
        return {"status": "success", "data": stats}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@router.get("/pdf/cache/list")
async def list_pdf_cache():
    """
    åˆ—å‡ºæ‰€æœ‰PDFç¼“å­˜

    Returns:
        {
            "status": "success",
            "data": [
                {
                    "id": int,
                    "file_name": str,
                    "file_path": str,
                    "file_size": int,
                    "char_count": int,
                    "processing_time": float,
                    "created_at": str,
                    "updated_at": str
                },
                ...
            ]
        }
    """
    try:
        repo = PDFRepository()
        docs = await repo.get_all_cached()
        return {
            "status": "success",
            "data": [
                {
                    "id": doc.id,
                    "file_name": doc.file_name,
                    "file_path": doc.file_path,
                    "file_size": doc.file_size,
                    "char_count": doc.char_count,
                    "processing_time": doc.processing_time,
                    "created_at": doc.created_at.isoformat(),
                    "updated_at": doc.updated_at.isoformat()
                }
                for doc in docs
            ]
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@router.delete("/pdf/cache/delete")
async def delete_pdf_cache(file_path: str):
    """
    åˆ é™¤æŒ‡å®šPDFçš„ç¼“å­˜

    Args:
        file_path: PDFæ–‡ä»¶è·¯å¾„ (å¦‚: data/knowledge/xxx.pdf)

    Returns:
        {
            "status": "success",
            "message": "ç¼“å­˜å·²åˆ é™¤"
        }
    """
    try:
        repo = PDFRepository()
        success = await repo.delete_by_path(file_path)
        if success:
            return {"status": "success", "message": "ç¼“å­˜å·²åˆ é™¤"}
        else:
            return {"status": "error", "message": "ç¼“å­˜ä¸å­˜åœ¨"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@router.delete("/pdf/cache/clear")
async def clear_pdf_cache():
    """
    æ¸…ç©ºæ‰€æœ‰PDFç¼“å­˜

    è­¦å‘Šï¼šæ­¤æ“ä½œä¸å¯é€†

    Returns:
        {
            "status": "success",
            "deleted_count": int
        }
    """
    try:
        repo = PDFRepository()
        count = await repo.clear_all()
        return {"status": "success", "deleted_count": count}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@router.post("/pdf/reindex")
async def rebuild_pdf_index():
    """
    é‡å»ºPDFç´¢å¼• (å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰PDF)

    æ³¨æ„ï¼šæ­¤æ“ä½œä¼šæ¸…é™¤æ‰€æœ‰PDFç¼“å­˜å¹¶é‡æ–°å¤„ç†ï¼Œè€—æ—¶è¾ƒé•¿

    Returns:
        {
            "status": "success",
            "message": "åå°ä»»åŠ¡å·²å¯åŠ¨"
        }
    """
    try:
        # TODO: å®ç°åå°ä»»åŠ¡é˜Ÿåˆ—
        return {"status": "success", "message": "åŠŸèƒ½å¼€å‘ä¸­"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

# ==========================================
# 8. çŸ¥è¯†åº“ç´¢å¼•ç®¡ç†æ¥å£
# ==========================================

@router.post("/index/rebuild")
async def rebuild_knowledge_base_index(request: Request):
    """
    é‡å»ºçŸ¥è¯†åº“ç´¢å¼• (æµå¼SSEå“åº”)

    SSEäº‹ä»¶ç±»å‹ï¼š
    - init: å¼€å§‹é‡å»º
    - progress: æ›´æ–°è¿›åº¦ {current, total, current_file, percentage}
    - step: é˜¶æ®µæç¤º {message, step}
    - complete: å®Œæˆç»Ÿè®¡ {message, stats}
    - error: é”™è¯¯ä¿¡æ¯ {message}
    - cancelled: å–æ¶ˆä¿¡æ¯ {message}
    """
    kb = getattr(request.app.state, "kb", None)
    if not kb:
        raise HTTPException(status_code=503, detail="çŸ¥è¯†åº“æœåŠ¡æœªå°±ç»ª")

    return StreamingResponse(
        kb.rebuild_index_stream(),
        media_type="text/event-stream"
    )

@router.get("/index/status")
async def get_index_status(request: Request):
    """
    è·å–ç´¢å¼•çŠ¶æ€

    Returns:
        {
            "status": "success",
            "data": {
                "is_rebuilding": bool,
                "progress": {
                    "current": int,
                    "total": int,
                    "current_file": str,
                    "percentage": float
                },
                "file_count": int,
                "last_rebuild_time": float | None
            }
        }
    """
    kb = getattr(request.app.state, "kb", None)
    if not kb:
        return {
            "status": "error",
            "message": "çŸ¥è¯†åº“æœåŠ¡æœªå°±ç»ª"
        }

    return {
        "status": "success",
        "data": {
            "is_rebuilding": kb.is_rebuilding,
            "progress": kb.progress,
            "file_count": kb.file_count,
            "last_rebuild_time": kb.last_rebuild_time
        }
    }

@router.post("/index/cancel")
async def cancel_index_rebuild(request: Request):
    """
    å–æ¶ˆç´¢å¼•é‡å»ºä»»åŠ¡

    Returns:
        {
            "status": "success",
            "message": "å–æ¶ˆè¯·æ±‚å·²å‘é€"
        }
    """
    kb = getattr(request.app.state, "kb", None)
    if not kb:
        return {
            "status": "error",
            "message": "çŸ¥è¯†åº“æœåŠ¡æœªå°±ç»ª"
        }

    if not kb.is_rebuilding:
        return {
            "status": "error",
            "message": "æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„é‡å»ºä»»åŠ¡"
        }

    kb.cancel_rebuild()
    return {
        "status": "success",
        "message": "å–æ¶ˆè¯·æ±‚å·²å‘é€"
    }

# ==========================================
# 9. LLM é…ç½®ç®¡ç†æ¥å£
# ==========================================

class LLMConfigRequest(BaseModel):
    provider: str
    api_key: str
    base_url: str
    model_name: str
    api_version: Optional[str] = None
    temperature: Optional[float] = 0.3
    is_enabled: Optional[bool] = True  # æ–°å¢ï¼šæ¥æ”¶å‰ç«¯å¼€å…³çŠ¶æ€

    # ==================== æ–°å¢ï¼šå›¾åƒè¯†åˆ«é…ç½® ====================
    image_enabled: Optional[bool] = True  # æ˜¯å¦å¯ç”¨å›¾åƒè¯†åˆ«åŠŸèƒ½
    image_model_name: Optional[str] = None  # å›¾åƒæ¨¡å‹åç§°ï¼ˆå¦‚ gpt-4-vision-previewï¼‰
    # ============================================================


class LLMConfigResponse(BaseModel):
    is_enabled: bool
    provider: str
    base_url: str
    model_name: str
    temperature: float
    test_status: str
    last_tested_at: Optional[str] = None
    api_key: Optional[str] = None  # æ·»åŠ API Keyå­—æ®µç”¨äºå‰ç«¯å¡«å……

    # ==================== æ–°å¢ï¼šå›¾åƒè¯†åˆ«é…ç½® ====================
    image_enabled: Optional[bool] = True
    image_model_name: Optional[str] = None
    # ============================================================

    model_config = {"exclude_unset": False}  # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½è¢«åºåˆ—åŒ–


@router.get("/config/llm")
async def get_llm_config():
    """è·å–å½“å‰ LLM é…ç½®"""
    if not BATCH_AVAILABLE:
        raise HTTPException(status_code=501, detail="æ•°æ®åº“ä¸å¯ç”¨")

    from src.database.connection import AsyncSessionLocal
    async with AsyncSessionLocal() as db:
        from src.database.crud import LLMConfigRepository
        repo = LLMConfigRepository(db)
        config = await repo.get_active_config()

        if not config:
            from src.config.loader import settings
            return LLMConfigResponse(
                is_enabled=False,
                provider="deepseek",
                base_url=settings.DEEPSEEK_BASE_URL,
                model_name=settings.DEEPSEEK_MODEL,
                temperature=0.3,
                test_status="never",
                last_tested_at=None,
                image_enabled=False,
                image_model_name=None
            )

        return LLMConfigResponse(
            is_enabled=config.is_enabled,
            provider=config.provider,
            base_url=config.base_url,
            model_name=config.model_name,
            temperature=config.temperature,
            test_status=config.test_status,
            last_tested_at=config.last_tested_at.isoformat() if config.last_tested_at else None,
            api_key=config.api_key,  # è¿”å›API Keyç”¨äºå‰ç«¯å¡«å……
            # ==================== æ–°å¢ï¼šå›¾åƒè¯†åˆ«é…ç½® ====================
            image_enabled=getattr(config, 'image_enabled', False),
            image_model_name=getattr(config, 'image_model_name', None)
            # ============================================================
        )


@router.post("/config/llm")
async def save_llm_config(config: LLMConfigRequest):
    """ä¿å­˜ LLM é…ç½®"""
    if not BATCH_AVAILABLE:
        raise HTTPException(status_code=501, detail="æ•°æ®åº“ä¸å¯ç”¨")

    from src.database.connection import AsyncSessionLocal
    async with AsyncSessionLocal() as db:
        from src.database.crud import LLMConfigRepository
        repo = LLMConfigRepository(db)
        saved_config = await repo.save_config(config.dict())

        return {
            "status": "success",
            "message": "é…ç½®å·²ä¿å­˜",
            "config_id": saved_config.id
        }


@router.post("/config/llm/test")
async def test_llm_connection(config: LLMConfigRequest):
    """æµ‹è¯• LLM è¿æ¥"""
    try:
        import httpx
        from langchain_openai import ChatOpenAI
        from langchain_core.messages import HumanMessage

        async_client = httpx.AsyncClient(verify=False, timeout=30.0)
        test_llm = ChatOpenAI(
            model=config.model_name,
            api_key=config.api_key,
            base_url=config.base_url,
            temperature=0.1,
            http_async_client=async_client,
            max_tokens=10,
        )

        response = await test_llm.ainvoke([HumanMessage(content="Hi")])
        await async_client.aclose()

        if response.content:
            return {
                "status": "success",
                "message": "è¿æ¥æˆåŠŸ",
                "response_preview": str(response.content[:50])
            }
        else:
            return {
                "status": "error",
                "message": "æ— å“åº”å†…å®¹"
            }

    except Exception as e:
        traceback.print_exc()
        return {
            "status": "error",
            "message": f"è¿æ¥å¤±è´¥: {str(e)}"
        }


@router.post("/config/llm/reload")
async def reload_llm_config(request: Request):
    """
    çƒ­é‡è½½ LLM é…ç½®ï¼ˆæ— éœ€é‡å¯æœåŠ¡ï¼‰

    é‡æ–°åˆå§‹åŒ–æ‰€æœ‰ Agentï¼Œä½¿æ–°é…ç½®ç«‹å³ç”Ÿæ•ˆ
    """
    try:
        from src.database.connection import AsyncSessionLocal
        from src.config.llm_loader import llm_config_loader
        from src.services.chat_agent import CustomsChatAgent
        from src.services.report_agent import ComplianceReporter

        # åŠ è½½æ–°é…ç½®
        async with AsyncSessionLocal() as db:
            llm_config = await llm_config_loader.load_config(db)

        # âœ… ä¿®å¤ï¼šæ›´æ–° app.state.llm_configï¼ˆå…³é”®ï¼ï¼‰
        # åŠŸèƒ½ä¸€ä¾èµ–æ­¤é…ç½®ï¼Œå¿…é¡»æ›´æ–°
        request.app.state.llm_config = llm_config

        # é‡æ–°åˆå§‹åŒ– Agent
        kb = request.app.state.kb
        request.app.state.agent = CustomsChatAgent(kb=kb, llm_config=llm_config)
        request.app.state.reporter = ComplianceReporter(kb=kb, llm_config=llm_config)

        return {
            "status": "success",
            "message": "é…ç½®å·²é‡æ–°åŠ è½½",
            "config": {
                "source": llm_config.get('source', 'unknown'),
                "provider": llm_config.get('source', 'unknown'),
                "model": llm_config['model'],
                "base_url": llm_config['base_url']
            }
        }
    except Exception as e:
        traceback.print_exc()
        return {
            "status": "error",
            "message": f"é‡è½½å¤±è´¥: {str(e)}",
            "detail": traceback.format_exc()[:500]
        }


@router.post("/config/llm/reset")
async def reset_llm_config():
    """é‡ç½®ä¸º .env é»˜è®¤é…ç½®"""
    if not BATCH_AVAILABLE:
        raise HTTPException(status_code=501, detail="æ•°æ®åº“ä¸å¯ç”¨")

    from src.database.connection import AsyncSessionLocal
    async with AsyncSessionLocal() as db:
        from src.database.crud import LLMConfigRepository
        repo = LLMConfigRepository(db)
        await repo.reset_to_env()

        return {
            "status": "success",
            "message": "å·²é‡ç½®ä¸º .env é»˜è®¤é…ç½®"
        }


@router.get("/config/llm/all")
async def get_all_llm_configs():
    """è·å–æ‰€æœ‰å·²ä¿å­˜çš„å‚å•†é…ç½®"""
    if not BATCH_AVAILABLE:
        raise HTTPException(status_code=501, detail="æ•°æ®åº“ä¸å¯ç”¨")

    from src.database.connection import AsyncSessionLocal
    async with AsyncSessionLocal() as db:
        from src.database.crud import LLMConfigRepository
        repo = LLMConfigRepository(db)
        configs = await repo.get_all_configs()

        # è¿”å›é…ç½®åˆ—è¡¨ï¼ˆéšè—API Keyï¼‰
        result = []
        for config in configs:
            result.append({
                "provider": config.provider,
                "is_enabled": config.is_enabled,
                "base_url": config.base_url,
                "model_name": config.model_name,
                "temperature": config.temperature,
                "test_status": config.test_status,
                "api_key_preview": config.api_key[:8] + "..." if config.api_key else "",
                "has_api_key": bool(config.api_key)
            })

        return {
            "status": "success",
            "configs": result
        }


@router.get("/config/llm/provider/{provider}")
async def get_provider_config(provider: str):
    """è·å–æŒ‡å®šå‚å•†çš„é…ç½®"""
    if not BATCH_AVAILABLE:
        raise HTTPException(status_code=501, detail="æ•°æ®åº“ä¸å¯ç”¨")

    from src.database.connection import AsyncSessionLocal
    async with AsyncSessionLocal() as db:
        from src.database.crud import LLMConfigRepository
        repo = LLMConfigRepository(db)
        config = await repo.get_config_by_provider(provider)

        if not config:
            return {
                "status": "not_found",
                "message": f"æœªæ‰¾åˆ° {provider} çš„é…ç½®"
            }

        return {
            "status": "success",
            "config": {
                "provider": config.provider,
                "is_enabled": config.is_enabled,
                "base_url": config.base_url,
                "model_name": config.model_name,
                "temperature": config.temperature,
                "api_version": config.api_version,
                "test_status": config.test_status,
                # è¿”å›å®Œæ•´API Keyç”¨äºå‰ç«¯å¡«å……
                "api_key": config.api_key
            }
        }


@router.post("/config/llm/activate/{provider}")
async def activate_provider_config(provider: str):
    """æ¿€æ´»æŒ‡å®šå‚å•†çš„é…ç½®"""
    if not BATCH_AVAILABLE:
        raise HTTPException(status_code=501, detail="æ•°æ®åº“ä¸å¯ç”¨")

    from src.database.connection import AsyncSessionLocal
    async with AsyncSessionLocal() as db:
        from src.database.crud import LLMConfigRepository
        repo = LLMConfigRepository(db)
        config = await repo.activate_provider(provider)

        if not config:
            raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ° {provider} çš„é…ç½®")

        # çƒ­é‡è½½é…ç½®
        from src.config.llm_loader import llm_config_loader
        llm_config = await llm_config_loader.load_config(db)

        return {
            "status": "success",
            "message": f"å·²åˆ‡æ¢åˆ° {provider}",
            "config": {
                "provider": provider,
                "model": llm_config['model'],
                "base_url": llm_config['base_url']
            }
        }


@router.get("/config/llm/models")
async def get_available_models(
    provider: str,
    api_key: str,
    base_url: str = None,
    api_version: str = None
):
    """
    è·å–æŒ‡å®šå‚å•†çš„æ¨¡å‹åˆ—è¡¨

    å‚æ•°ï¼š
    - provider: å‚å•†åç§° (deepseek, openai, qwen, zhipu, siliconflow, azure, custom)
    - api_key: APIå¯†é’¥
    - base_url: è‡ªå®šä¹‰base_urlï¼ˆç”¨äºazureå’Œcustom providerï¼‰
    - api_version: APIç‰ˆæœ¬ï¼ˆAzureç‰¹æœ‰ï¼Œå¦‚ï¼š2024-03-01-previewï¼‰
    """
    try:
        import httpx

        # 1. æ™ºè°±GLMï¼šæ— æ¨¡å‹åˆ—è¡¨APIï¼Œè¿”å›ç¡¬ç¼–ç åˆ—è¡¨
        if provider == "zhipu":
            models = [
                "glm-4.7",
                "glm-4-turbo",
                "glm-4-plus",
                "glm-4-air",
                "glm-4-flash"
            ]
            return {
                "status": "success",
                "models": models,
                "source": "hardcoded"
            }

        # 2. Azure OpenAIï¼šç‰¹æ®Šå¤„ç†ï¼ˆapi-key header + api-versionå‚æ•°ï¼‰
        if provider == "azure":
            if not base_url:
                return {
                    "status": "error",
                    "message": "Azureéœ€è¦æä¾›Endpointï¼ˆå¦‚ï¼šhttps://your-resource.openai.azure.comï¼‰"
                }
            if not api_version:
                api_version = "2024-03-01-preview"  # é»˜è®¤ç‰ˆæœ¬

            url = f"{base_url.rstrip('/')}/openai/models?api-version={api_version}"
            headers = {"api-key": api_key}  # Azureä½¿ç”¨api-key header

            async with httpx.AsyncClient(verify=False, timeout=30.0) as client:
                response = await client.get(url, headers=headers)

                if response.status_code == 200:
                    data = response.json()
                    models = [item["id"] for item in data.get("data", [])]
                    return {
                        "status": "success",
                        "models": models,
                        "source": "api"
                    }
                elif response.status_code == 401:
                    return {"status": "error", "message": "API Key æ— æ•ˆ"}
                else:
                    return {
                        "status": "error",
                        "message": f"APIè°ƒç”¨å¤±è´¥ (HTTP {response.status_code})"
                    }

        # 3. å…¶ä»–å‚å•†ï¼šä½¿ç”¨æ ‡å‡†OpenAIå…¼å®¹æ ¼å¼
        # å¦‚æœç”¨æˆ·æä¾›äº† base_urlï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æä¾›çš„
        if base_url and base_url.strip():
            url = f"{base_url.rstrip('/')}/models"
        else:
            # ä½¿ç”¨é»˜è®¤çš„ provider_urlsï¼ˆæ ¹æ®å®˜æ–¹æ–‡æ¡£é…ç½®ï¼‰
            provider_urls = {
                "deepseek": "https://api.deepseek.com/v1/models",
                "openai": "https://api.openai.com/v1/models",
                "qwen": "https://dashscope.aliyuncs.com/compatible-mode/v1/models",
                "siliconflow": "https://api.siliconflow.cn/v1/models",  # å›½å†…ä¸»åŸŸåï¼ˆæ–‡æ¡£ç¬¬24è¡Œï¼‰
                "zhipu": "https://open.bigmodel.cn/api/paas/v4/models",
            }

            if provider not in provider_urls:
                return {
                    "status": "error",
                    "message": f"ä¸æ”¯æŒçš„æœåŠ¡å•†: {provider}"
                }

            url = provider_urls[provider]
            if not url:
                return {
                    "status": "error",
                    "message": "è‡ªå®šä¹‰æœåŠ¡å•†éœ€è¦æä¾›base_url"
                }

        # 4. æ ‡å‡†OpenAIæ ¼å¼è°ƒç”¨ï¼ˆAuthorization: Bearerï¼‰
        async with httpx.AsyncClient(verify=False, timeout=30.0) as client:
            response = await client.get(
                url,
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                }
            )

            if response.status_code == 200:
                data = response.json()
                # OpenAIå…¼å®¹æ ¼å¼ï¼šdata[].id
                models = [item["id"] for item in data.get("data", [])]

                return {
                    "status": "success",
                    "models": models,
                    "source": "api"
                }
            elif response.status_code == 401:
                return {
                    "status": "error",
                    "message": "API Key æ— æ•ˆæˆ–å·²è¿‡æœŸ"
                }
            else:
                return {
                    "status": "error",
                    "message": f"APIè°ƒç”¨å¤±è´¥ (HTTP {response.status_code})"
                }

    except httpx.TimeoutException:
        return {
            "status": "error",
            "message": "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        }
    except Exception as e:
        traceback.print_exc()
        return {
            "status": "error",
            "message": f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}"
        }


# ==================== å›¾åƒè¯†åˆ«é…ç½® API ====================

class ImageConfigRequest(BaseModel):
    """å›¾åƒé…ç½®è¯·æ±‚æ¨¡å‹"""
    provider: str
    api_key: str
    base_url: Optional[str] = None
    model_name: str
    api_version: Optional[str] = None
    endpoint: Optional[str] = None
    temperature: Optional[float] = 0.1
    max_tokens: Optional[int] = 16384
    is_enabled: Optional[bool] = True
    description: Optional[str] = None


class ImageConfigResponse(BaseModel):
    """å›¾åƒé…ç½®å“åº”æ¨¡å‹"""
    id: Optional[int] = None
    provider: str
    is_enabled: bool
    api_key: Optional[str] = None  # âœ… æ·»åŠ  API Key å­—æ®µ
    base_url: Optional[str] = None
    model_name: str
    api_version: Optional[str] = None
    endpoint: Optional[str] = None
    temperature: float
    max_tokens: int
    test_status: str
    description: Optional[str] = None


@router.get("/config/image")
async def get_image_config():
    """è·å–å½“å‰å›¾åƒè¯†åˆ«é…ç½®"""
    if not BATCH_AVAILABLE:
        # é™çº§ï¼šè¿”å› .env é…ç½®
        from src.config.image_loader import image_config_loader
        config = image_config_loader.load_from_env()

        return ImageConfigResponse(
            provider=config["provider"],
            is_enabled=config["is_enabled"],
            api_key=config.get("api_key"),  # âœ… æ·»åŠ  API Key
            base_url=config.get("base_url"),
            model_name=config["model_name"],
            api_version=config.get("api_version"),
            endpoint=config.get("endpoint"),
            temperature=config["temperature"],
            max_tokens=config["max_tokens"],
            test_status="never"
        )

    from src.database.connection import AsyncSessionLocal
    async with AsyncSessionLocal() as db:
        from src.database.image_config_crud import ImageConfigRepository
        repo = ImageConfigRepository(db)

        # ä¼˜å…ˆè·å–å¯ç”¨çš„é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™è·å–æœ€æ–°ä¿å­˜çš„é…ç½®
        config = await repo.get_active_config()
        if not config:
            config = await repo.get_latest_config()

        if config:
            return ImageConfigResponse(
                id=config.id,
                provider=config.provider,
                is_enabled=config.is_enabled,  # å‘Šè¯‰å‰ç«¯é…ç½®æ˜¯å¦å¯ç”¨
                api_key=config.api_key,  # âœ… æ·»åŠ  API Key
                base_url=config.base_url,
                model_name=config.model_name,
                api_version=config.api_version,
                endpoint=config.endpoint,
                temperature=config.temperature,
                max_tokens=config.max_tokens,
                test_status=config.test_status,
                description=config.description
            )
        else:
            # æ²¡æœ‰æ•°æ®åº“é…ç½®ï¼Œè¿”å› .env é»˜è®¤é…ç½®
            from src.config.image_loader import image_config_loader
            env_config = image_config_loader.load_from_env()

            return ImageConfigResponse(
                provider=env_config["provider"],
                is_enabled=False,
                api_key=env_config.get("api_key"),  # âœ… æ·»åŠ  API Key
                base_url=env_config.get("base_url"),
                model_name=env_config["model_name"],
                api_version=env_config.get("api_version"),
                endpoint=env_config.get("endpoint"),
                temperature=env_config["temperature"],
                max_tokens=env_config["max_tokens"],
                test_status="never"
            )


@router.post("/config/image")
async def save_image_config(config: ImageConfigRequest):
    """ä¿å­˜å›¾åƒè¯†åˆ«é…ç½®"""
    print(f"\n{'='*80}")
    print(f"ğŸ“ [Image Config] ä¿å­˜å›¾åƒè¯†åˆ«é…ç½®")
    print(f"{'='*80}")
    print(f"Provider: {config.provider}")
    print(f"API Key: {config.api_key[:15]}...{config.api_key[-5:] if len(config.api_key) > 20 else '***'} (é•¿åº¦: {len(config.api_key)})")
    print(f"Base URL: {config.base_url}")
    print(f"Model: {config.model_name}")
    print(f"Enabled: {config.is_enabled}")
    print(f"{'='*80}\n")

    if not BATCH_AVAILABLE:
        raise HTTPException(status_code=501, detail="æ•°æ®åº“ä¸å¯ç”¨")

    from src.database.connection import AsyncSessionLocal
    async with AsyncSessionLocal() as db:
        from src.database.image_config_crud import ImageConfigRepository
        repo = ImageConfigRepository(db)

        # è½¬æ¢ä¸ºå­—å…¸
        config_data = config.dict(exclude_unset=True)

        # ä¿å­˜é…ç½®
        saved_config = await repo.create_or_update(config_data)

        print(f"\n{'='*80}")
        print(f"âœ… [Image Config] é…ç½®ä¿å­˜å®Œæˆ")
        print(f"{'='*80}")
        print(f"ID: {saved_config.id}")
        print(f"Provider: {saved_config.provider}")
        print(f"Model: {saved_config.model_name}")
        print(f"Enabled: {saved_config.is_enabled}")
        print(f"{'='*80}\n")

        return {
            "status": "success",
            "message": "å›¾åƒè¯†åˆ«é…ç½®å·²ä¿å­˜",
            "config": repo.to_dict(saved_config)
        }


@router.post("/config/image/test")
async def test_image_connection(config: ImageConfigRequest):
    """æµ‹è¯•å›¾åƒè¯†åˆ«é…ç½®è¿æ¥ï¼ˆçœŸå®APIè°ƒç”¨ï¼‰"""
    try:
        import httpx

        # éªŒè¯å¿…è¦å‚æ•°
        if not config.api_key or len(config.api_key) < 10:
            return {
                "status": "error",
                "message": "API Key æ— æ•ˆæˆ–å¤ªçŸ­"
            }

        if not config.model_name:
            return {
                "status": "error",
                "message": "è¯·æŒ‡å®šæ¨¡å‹åç§°"
            }

        # æ ¹æ®providerçœŸå®æµ‹è¯•API
        if config.provider == "gemini":
            # æµ‹è¯• Gemini API
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{config.model_name}:generateContent?key={config.api_key}"
            test_payload = {
                "contents": [{
                    "parts": [{"text": "Hi"}]
                }]
            }

            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(url, json=test_payload)

                if response.status_code == 200:
                    return {
                        "status": "success",
                        "message": f"âœ… Gemini è¿æ¥æˆåŠŸ ({config.model_name})"
                    }
                elif response.status_code == 429:
                    return {
                        "status": "error",
                        "message": "âŒ API é…é¢å·²ç”¨å®Œæˆ–è¯·æ±‚é¢‘ç‡è¶…é™ (429)"
                    }
                elif response.status_code == 401:
                    return {
                        "status": "error",
                        "message": "âŒ API Key æ— æ•ˆ (401)"
                    }
                else:
                    return {
                        "status": "error",
                        "message": f"âŒ API è°ƒç”¨å¤±è´¥ (HTTP {response.status_code})"
                    }

        elif config.provider == "azure":
            # æµ‹è¯• Azure OpenAI API
            if not config.endpoint:
                return {
                    "status": "error",
                    "message": "âŒ Azure Endpoint æœªé…ç½®"
                }

            url = f"{config.endpoint}/openai/deployments/{config.model_name}/chat/completions?api-version={config.api_version}"
            headers = {
                "api-key": config.api_key,
                "Content-Type": "application/json"
            }
            test_payload = {
                "messages": [{"role": "user", "content": "Hi"}],
                "max_tokens": 10
            }

            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(url, json=test_payload, headers=headers)

                if response.status_code == 200:
                    return {
                        "status": "success",
                        "message": f"âœ… Azure è¿æ¥æˆåŠŸ ({config.model_name})"
                    }
                elif response.status_code == 401:
                    return {
                        "status": "error",
                        "message": "âŒ API Key æ— æ•ˆ (401)"
                    }
                else:
                    return {
                        "status": "error",
                        "message": f"âŒ API è°ƒç”¨å¤±è´¥ (HTTP {response.status_code})"
                    }

        else:
            return {
                "status": "success",
                "message": f"âš ï¸ {config.provider} é…ç½®å·²ä¿å­˜ï¼ˆæ— æ³•è‡ªåŠ¨æµ‹è¯•ï¼‰"
            }

    except httpx.TimeoutException:
        return {
            "status": "error",
            "message": "âŒ è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}"
        }


@router.post("/config/image/reset")
async def reset_image_config():
    """é‡ç½®ä¸º .env é»˜è®¤é…ç½®"""
    if not BATCH_AVAILABLE:
        raise HTTPException(status_code=501, detail="æ•°æ®åº“ä¸å¯ç”¨")

    from src.database.connection import AsyncSessionLocal
    async with AsyncSessionLocal() as db:
        from src.database.image_config_crud import ImageConfigRepository
        repo = ImageConfigRepository(db)
        await repo.disable_all()

        return {
            "status": "success",
            "message": "å·²é‡ç½®ä¸º .env é»˜è®¤é…ç½®"
        }


@router.post("/config/image/reload")
async def reload_image_config():
    """çƒ­é‡è½½å›¾åƒè¯†åˆ«é…ç½®"""
    if not BATCH_AVAILABLE:
        raise HTTPException(status_code=501, detail="æ•°æ®åº“ä¸å¯ç”¨")

    from src.database.connection import AsyncSessionLocal
    async with AsyncSessionLocal() as db:
        from src.database.image_config_crud import ImageConfigRepository
        from src.config.image_loader import image_config_loader

        repo = ImageConfigRepository(db)
        config = await repo.get_active_config()

        if config:
            config_dict = repo.to_dict(config)
            image_config_loader.set_config(config_dict)

            return {
                "status": "success",
                "message": "é…ç½®å·²é‡è½½",
                "config": {
                    "provider": config_dict["provider"],
                    "model": config_dict["model_name"],
                    "enabled": config_dict["is_enabled"]
                }
            }
        else:
            # ä½¿ç”¨ .env é…ç½®
            env_config = image_config_loader.load_from_env()
            image_config_loader.set_config(env_config)

            return {
                "status": "success",
                "message": "å·²é‡è½½ .env é»˜è®¤é…ç½®",
                "config": {
                    "provider": env_config["provider"],
                    "model": env_config["model_name"],
                    "enabled": False
                }
            }


@router.get("/config/image/provider/{provider}")
async def get_image_provider_config(provider: str):
    """è·å–æŒ‡å®šæœåŠ¡å•†çš„å›¾åƒé…ç½®ï¼ˆè¿”å›å®Œæ•´API Keyç”¨äºå‰ç«¯å¡«å……ï¼‰"""
    if not BATCH_AVAILABLE:
        raise HTTPException(status_code=501, detail="æ•°æ®åº“ä¸å¯ç”¨")

    from src.database.connection import AsyncSessionLocal
    async with AsyncSessionLocal() as db:
        from src.database.image_config_crud import ImageConfigRepository
        repo = ImageConfigRepository(db)
        config = await repo.get_by_provider(provider)

        if config:
            return {
                "status": "success",
                "config": {
                    "provider": config.provider,
                    "api_key": config.api_key,  # è¿”å›å®Œæ•´API Key
                    "base_url": config.base_url,
                    "model_name": config.model_name,
                    "endpoint": config.endpoint,
                    "api_version": config.api_version,
                    "has_api_key": bool(config.api_key)
                }
            }
        else:
            return {
                "status": "error",
                "message": f"æœªæ‰¾åˆ° {provider} çš„é…ç½®"
            }


@router.get("/config/image/models")
async def get_image_models(
    provider: str,
    api_key: str,
    base_url: str = None,
    api_version: str = None
):
    """
    è·å–å›¾åƒè¯†åˆ«æ¨¡å‹åˆ—è¡¨ï¼ˆå¤ç”¨LLMé…ç½®çš„ç›¸åŒé€»è¾‘ï¼‰
    å‚æ•°ä¸ LLM é…ç½®å®Œå…¨ç›¸åŒ
    """
    # ========== è¯¦ç»†è°ƒè¯•ä¿¡æ¯ ==========
    print("\n" + "="*80)
    print(f"ğŸ” [å›¾åƒæ¨¡å‹åˆ—è¡¨] å¼€å§‹è·å– {provider} çš„æ¨¡å‹åˆ—è¡¨")
    print("="*80)
    print(f"ğŸ“Œ å‚æ•°ä¿¡æ¯:")
    print(f"  - provider: {provider}")
    print(f"  - api_key: {api_key[:15]}...{api_key[-5:] if len(api_key) > 20 else '***'} (é•¿åº¦: {len(api_key)})")
    print(f"  - base_url: {base_url}")
    print(f"  - api_version: {api_version}")
    print("="*80 + "\n")

    try:
        import httpx

        # æ™ºè°±GLMï¼šä½¿ç”¨ç¡¬ç¼–ç åˆ—è¡¨
        if provider == "zhipu":
            print(f"âœ… [æ™ºè°±GLM] ä½¿ç”¨ç¡¬ç¼–ç æ¨¡å‹åˆ—è¡¨")
            return {
                "status": "success",
                "models": ["glm-4v", "glm-4v-plus", "glm-4v-flash"],
                "source": "builtin"
            }

        # Azureï¼šéœ€è¦endpointå’ŒapiVersion
        elif provider == "azure":
            print(f"ğŸ”§ [Azure] æ£€æŸ¥å‚æ•°...")
            if not base_url:
                print(f"âŒ [Azure] ç¼ºå°‘ base_url å‚æ•°")
                return {
                    "status": "error",
                    "message": "Azureéœ€è¦ base_url å‚æ•°"
                }

            url = f"{base_url}/openai/deployments?api-version={api_version}"
            headers = {"api-key": api_key}

            print(f"ğŸ“¡ [Azure] å‘é€è¯·æ±‚:")
            print(f"  - URL: {url}")
            print(f"  - Headers: api-key={headers['api-key'][:10]}...***")

            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url, headers=headers)

                print(f"ğŸ“¥ [Azure] å“åº”çŠ¶æ€: HTTP {response.status_code}")
                print(f"ğŸ“„ [Azure] å“åº”å†…å®¹: {response.text[:500]}")

                if response.status_code == 200:
                    data = response.json()
                    models = [item["id"] for item in data.get("data", [])]
                    print(f"âœ… [Azure] æˆåŠŸè·å– {len(models)} ä¸ªæ¨¡å‹")
                    return {
                        "status": "success",
                        "models": models,
                        "source": "api"
                    }
                else:
                    print(f"âŒ [Azure] è¯·æ±‚å¤±è´¥")
                    return {
                        "status": "error",
                        "message": f"APIè°ƒç”¨å¤±è´¥ (HTTP {response.status_code})\nå“åº”: {response.text[:200]}"
                    }

        # å…¶ä»–å‚å•†ï¼šè°ƒç”¨æ ‡å‡†OpenAIå…¼å®¹API
        else:
            # æ²¡æœ‰API Keyæ—¶çš„å¤„ç†
            if not api_key:
                print(f"âš ï¸  [{provider.upper()}] æœªæä¾›API Keyï¼Œä½¿ç”¨é¢„è®¾æ¨¡å‹åˆ—è¡¨")

                # è¿”å›é¢„è®¾åˆ—è¡¨ï¼ˆæ›´æ–°ç¡…åŸºæµåŠ¨å›¾åƒæ¨¡å‹ï¼ŒåŸºäºå®˜æ–¹æ–‡æ¡£ï¼‰
                provider_presets = {
                    "deepseek": ["deepseek-chat", "deepseek-coder", "deepseek-reasoner"],
                    "openai": ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"],
                    "qwen": ["qwen-turbo", "qwen-plus", "qwen-max", "qwen-max-longcontext", "qwen-vl-max", "qwen2.5-vl-7b-instruct", "qwen2.5-vl-72b-instruct"],
                    "siliconflow": [
                        # ç¡…åŸºæµåŠ¨æ”¯æŒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆåŸºäºå®˜æ–¹æ–‡æ¡£ï¼‰
                        "Qwen/Qwen2-VL-7B-Instruct",
                        "Qwen/Qwen2-VL-72B-Instruct",
                        "Qwen/Qwen2.5-VL-7B-Instruct",
                        "Qwen/Qwen2.5-VL-32B-Instruct",
                        "Qwen/Qwen2.5-VL-72B-Instruct",
                        "Qwen/Qwen3-VL-32B-Instruct",
                        "OpenGVLab/InternVL2-Llama3-76B",
                        "deepseek-ai/deepseek-vl-7b"
                    ],
                    "zhipu": ["glm-4v", "glm-4v-plus", "glm-4v-flash"]
                }

                if provider in provider_presets:
                    models = provider_presets[provider]
                    print(f"âœ… [{provider.upper()}] è¿”å›é¢„è®¾æ¨¡å‹åˆ—è¡¨ ({len(models)} ä¸ªæ¨¡å‹)")
                    for i, model in enumerate(models, 1):
                        print(f"   {i}. {model}")
                    return {
                        "status": "success",
                        "models": models,
                        "source": "builtin"
                    }
                else:
                    print(f"âŒ [{provider.upper()}] æ— é¢„è®¾æ¨¡å‹åˆ—è¡¨")
                    return {
                        "status": "error",
                        "message": "è¯·å…ˆè¾“å…¥ API Key"
                    }

            # æœ‰API Keyæ—¶çš„å¤„ç†
            print(f"ğŸ”‘ [{provider.upper()}] å·²æä¾›API Keyï¼Œå°è¯•ä»APIè·å–æ¨¡å‹åˆ—è¡¨")

            # å¦‚æœç”¨æˆ·æä¾›äº† base_urlï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æä¾›çš„
            if base_url and base_url.strip():
                url = f"{base_url.rstrip('/')}/models"
                print(f"ğŸ“ ä½¿ç”¨ç”¨æˆ·æä¾›çš„ base_url")
            else:
                # ä½¿ç”¨é»˜è®¤çš„ provider_urlsï¼ˆæ ¹æ®å®˜æ–¹æ–‡æ¡£é…ç½®ï¼‰
                provider_urls = {
                    "deepseek": "https://api.deepseek.com/v1/models",
                    "openai": "https://api.openai.com/v1/models",
                    "qwen": "https://dashscope.aliyuncs.com/compatible-mode/v1/models",
                    "siliconflow": "https://api.siliconflow.cn/v1/models",  # å›½å†…ä¸»åŸŸåï¼ˆæ–‡æ¡£ç¬¬24è¡Œï¼‰
                    "zhipu": "https://open.bigmodel.cn/api/paas/v4/models",
                }

                url = provider_urls.get(provider)
                print(f"ğŸ“ ä½¿ç”¨é¢„è®¾çš„ provider_urls")
                if not url:
                    print(f"âŒ ä¸æ”¯æŒçš„æœåŠ¡å•†: {provider}")
                    return {
                        "status": "error",
                        "message": f"ä¸æ”¯æŒçš„æœåŠ¡å•†: {provider}"
                    }

            # ========== æ„å»ºè¯·æ±‚ ==========
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }

            print("\n" + "â”€"*80)
            print(f"ğŸ“¡ [{provider.upper()}] å‘é€HTTPè¯·æ±‚")
            print("â”€"*80)
            print(f"è¯·æ±‚æ–¹æ³•: GET")
            print(f"è¯·æ±‚URL: {url}")
            print(f"è¯·æ±‚Headers:")
            print(f"  - Authorization: Bearer {api_key[:15]}...{api_key[-5:] if len(api_key) > 20 else '***'}")
            print(f"  - Content-Type: {headers['Content-Type']}")
            print(f"  - é•¿åº¦: {len(api_key)} å­—ç¬¦")
            print("â”€"*80 + "\n")

            # 4. æ ‡å‡†OpenAIæ ¼å¼è°ƒç”¨
            try:
                async with httpx.AsyncClient(verify=False, timeout=30.0) as client:
                    response = await client.get(url, headers=headers)

                    print("\n" + "â”€"*80)
                    print(f"ğŸ“¥ [{provider.upper()}] æ”¶åˆ°HTTPå“åº”")
                    print("â”€"*80)
                    print(f"å“åº”çŠ¶æ€ç : HTTP {response.status_code}")
                    print(f"å“åº”å¤´:")
                    for key, value in response.headers.items():
                        if key.lower() in ['authorization', 'set-cookie']:
                            continue  # è·³è¿‡æ•æ„Ÿä¿¡æ¯
                        print(f"  - {key}: {value}")
                    print(f"å“åº”å†…å®¹ç±»å‹: {response.headers.get('content-type', 'unknown')}")
                    print(f"å“åº”å†…å®¹é•¿åº¦: {len(response.content)} å­—èŠ‚")
                    print("â”€"*80)

                    # æ‰“å°å“åº”å†…å®¹ï¼ˆæˆªå–å‰500å­—ç¬¦ï¼‰
                    response_text = response.text
                    print(f"\nğŸ“„ å“åº”å†…å®¹ (å‰500å­—ç¬¦):")
                    print("â”€"*80)
                    print(response_text[:500])
                    if len(response_text) > 500:
                        print(f"\n... (è¿˜æœ‰ {len(response_text) - 500} å­—ç¬¦)")
                    print("â”€"*80 + "\n")

                    if response.status_code == 200:
                        data = response.json()
                        models = [item["id"] for item in data.get("data", [])]
                        print(f"âœ… [{provider.upper()}] æˆåŠŸè·å– {len(models)} ä¸ªæ¨¡å‹")
                        if len(models) <= 20:
                            # å¦‚æœæ¨¡å‹ä¸å¤šï¼Œå…¨éƒ¨æ‰“å°
                            for i, model in enumerate(models, 1):
                                print(f"   {i}. {model}")
                        else:
                            # å¦‚æœæ¨¡å‹å¤ªå¤šï¼Œåªæ‰“å°å‰10ä¸ªå’Œå5ä¸ª
                            print(f"   å‰10ä¸ªæ¨¡å‹:")
                            for i in range(10):
                                print(f"   {i+1}. {models[i]}")
                            print(f"   ... è¿˜æœ‰ {len(models) - 15} ä¸ªæ¨¡å‹")
                            print(f"   æœ€å5ä¸ªæ¨¡å‹:")
                            for i in range(5):
                                print(f"   {len(models)-4+i}. {models[-5+i]}")

                        return {
                            "status": "success",
                            "models": models,
                            "source": "api"
                        }
                    elif response.status_code == 401:
                        print(f"âŒ [{provider.upper()}] API Key è®¤è¯å¤±è´¥ (HTTP 401)")
                        return {
                            "status": "error",
                            "message": "API Key æ— æ•ˆæˆ–å·²è¿‡æœŸ (HTTP 401)"
                        }
                    elif response.status_code == 404:
                        print(f"âŒ [{provider.upper()}] APIç«¯ç‚¹ä¸å­˜åœ¨ (HTTP 404)")
                        print(f"å¯èƒ½çš„åŸå› :")
                        print(f"  1. Base URLé…ç½®é”™è¯¯: {base_url}")
                        print(f"  2. è¯¥æœåŠ¡å•†ä¸æ”¯æŒæ¨¡å‹åˆ—è¡¨API")
                        print(f"  3. æœåŠ¡å•†ç«¯ç‚¹å·²å˜æ›´")
                        return {
                            "status": "error",
                            "message": f"APIç«¯ç‚¹ä¸å­˜åœ¨ (HTTP 404)\n\nè¯·æ£€æŸ¥ï¼š\n1. Base URLæ˜¯å¦æ­£ç¡®ï¼š{base_url}\n2. è¯¥æœåŠ¡å•†æ˜¯å¦æ”¯æŒæ¨¡å‹åˆ—è¡¨API\n3. æŸ¥çœ‹æ§åˆ¶å°è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯"
                        }
                    else:
                        print(f"âŒ [{provider.upper()}] APIè°ƒç”¨å¤±è´¥ (HTTP {response.status_code})")
                        return {
                            "status": "error",
                            "message": f"APIè°ƒç”¨å¤±è´¥ (HTTP {response.status_code})\n\nå“åº”å†…å®¹:\n{response_text[:300]}\n\nè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®"
                        }

            except httpx.TimeoutException as e:
                print(f"â±ï¸  [{provider.upper()}] è¯·æ±‚è¶…æ—¶")
                print(f"é”™è¯¯è¯¦æƒ…: {e}")
                return {
                    "status": "error",
                    "message": "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
                }
            except Exception as e:
                print(f"ğŸ’¥ [{provider.upper()}] è¯·æ±‚å¼‚å¸¸")
                print(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
                print(f"å¼‚å¸¸ä¿¡æ¯: {str(e)}")
                import traceback
                print(f"å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}")
                return {
                    "status": "error",
                    "message": f"è¯·æ±‚å¼‚å¸¸: {type(e).__name__}: {str(e)}"
                }
    except Exception as e:
        print(f"ğŸ’¥ [å›¾åƒæ¨¡å‹åˆ—è¡¨] é¡¶çº§å¼‚å¸¸æ•è·")
        print(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        print(f"å¼‚å¸¸ä¿¡æ¯: {str(e)}")
        import traceback
        print(f"å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}")
        return {
            "status": "error",
            "message": f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {type(e).__name__}: {str(e)}"
        }





"""
å¿«é€Ÿé‡å»ºFAISSç´¢å¼•ï¼ˆåªåŒ…å«çœŸå®æ–‡æ¡£ï¼‰
"""
import asyncio
import sys
import io
from pathlib import Path

# è®¾ç½®UTF-8è¾“å‡º
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from src.services.knowledge_base import KnowledgeBase
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS


async def rebuild():
    """é‡å»ºç´¢å¼•"""
    print("=" * 70)
    print("é‡å»ºFAISSç´¢å¼•ï¼ˆä»…åŒ…å«çœŸå®æ–‡æ¡£ï¼‰")
    print("=" * 70)

    # 1. åˆå§‹åŒ–KBï¼ˆåŠ è½½PDFï¼‰
    print("\næ­¥éª¤1: åŠ è½½PDFæ–‡æ¡£...")
    kb = KnowledgeBase()
    pdf_docs = await kb._process_pdfs()
    print(f"âœ… åŠ è½½äº† {len(pdf_docs)} ä¸ªPDFæ–‡æ¡£")

    # 2. åŠ è½½txtæ–‡ä»¶ï¼ˆåªæœ‰2ä¸ªçœŸå®æ–‡ä»¶ï¼‰
    print("\næ­¥éª¤2: åŠ è½½æ–‡æœ¬æ–‡ä»¶...")
    data_path = Path("data/knowledge")
    loaders = [
        DirectoryLoader(str(data_path), glob="*.txt", loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"}),
    ]

    txt_docs = []
    for loader in loaders:
        try:
            txt_docs.extend(loader.load())
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å‡ºé”™: {e}")

    print(f"âœ… åŠ è½½äº† {len(txt_docs)} ä¸ªæ–‡æœ¬æ–‡ä»¶")
    for doc in txt_docs:
        print(f"   - {Path(doc.metadata['source']).name}")

    # 3. åˆå¹¶
    all_docs = txt_docs + pdf_docs
    print(f"\næ­¥éª¤3: åˆå¹¶æ–‡æ¡£ï¼Œæ€»è®¡ {len(all_docs)} ä¸ª")

    # 4. åˆ‡åˆ†ï¼ˆä¼˜åŒ–åˆ†éš”ç¬¦ï¼Œé¿å…åœ¨åˆ†å·å¤„åˆ‡åˆ†ï¼‰
    print("\næ­¥éª¤4: åˆ‡åˆ†æ–‡æ¡£...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=150,
        separators=["ã€‚\n", "ï¼\n", "ï¼Ÿ\n", "\n\n\n", "\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
    )
    chunks = splitter.split_documents(all_docs)
    print(f"âœ… åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")

    # ğŸ”¥ è¿‡æ»¤æ‰å°äº50å­—ç¬¦çš„ä½è´¨é‡chunk
    original_count = len(chunks)
    chunks = [c for c in chunks if len(c.page_content) >= 50]
    filtered_count = original_count - len(chunks)
    print(f"ğŸ“„ è¿‡æ»¤å°chunk: {original_count} â†’ {len(chunks)} (è¿‡æ»¤äº†{filtered_count}ä¸ªå°ç‰‡æ®µ)")

    # ç»Ÿè®¡
    pdf_chunks = sum(1 for c in chunks if c.metadata.get('file_type') == 'pdf')
    txt_chunks = len(chunks) - pdf_chunks
    print(f"   PDFç‰‡æ®µ: {pdf_chunks}")
    print(f"   TXTç‰‡æ®µ: {txt_chunks}")

    # 5. åˆ›å»ºç´¢å¼•
    print("\næ­¥éª¤5: åˆ›å»ºFAISSç´¢å¼•...")
    vector_store = FAISS.from_documents(chunks, kb.embeddings)
    print("âœ… å‘é‡åŒ–å®Œæˆ")

    # 6. ä¿å­˜
    print("\næ­¥éª¤6: ä¿å­˜ç´¢å¼•...")
    import shutil
    import os

    vector_db_path = kb.base_dir / "config" / "faiss_index_local"
    temp_path = kb.base_dir / "temp_faiss"

    if temp_path.exists():
        shutil.rmtree(temp_path)

    vector_store.save_local(str(temp_path))

    if vector_db_path.exists():
        shutil.rmtree(vector_db_path)
    vector_db_path.mkdir(parents=True, exist_ok=True)

    for file_name in os.listdir(temp_path):
        shutil.move(str(temp_path / file_name), str(vector_db_path / file_name))

    shutil.rmtree(temp_path)
    print(f"âœ… ç´¢å¼•å·²ä¿å­˜")

    print("\n" + "=" * 70)
    print("é‡å»ºå®Œæˆï¼")
    print(f"æ€»æ–‡æ¡£: {len(all_docs)} (TXT: {len(txt_docs)}, PDF: {len(pdf_docs)})")
    print(f"æ€»ç‰‡æ®µ: {len(chunks)} (TXT: {txt_chunks}, PDF: {pdf_chunks})")
    print("=" * 70)


if __name__ == "__main__":
    asyncio.run(rebuild())

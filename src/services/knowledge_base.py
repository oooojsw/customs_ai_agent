import os
import shutil
import asyncio
import json
import numpy as np
from pathlib import Path
from typing import List, AsyncGenerator
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

# PDFå¤„ç†ç›¸å…³
from src.services.pdf_service import PDFService, PDFProcessingError
from src.database.pdf_repository import PDFRepository

class KnowledgeBase:
    def __init__(self, process_pdfs: bool = True):
        # 1. å®šä¹‰ç»å¯¹è·¯å¾„
        self.base_dir = Path(__file__).resolve().parent.parent.parent
        self.data_path = self.base_dir / "data" / "knowledge"

        # å‘é‡æ•°æ®åº“æœ€ç»ˆä¿å­˜ç›®å½•
        self.vector_db_path = self.base_dir / "config" / "faiss_index_local"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.vector_db_path.mkdir(parents=True, exist_ok=True)

        # PDFå¤„ç†é…ç½®
        self.process_pdfs = process_pdfs
        self.pdf_service = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.pdf_repo = PDFRepository() if process_pdfs else None

        # ç´¢å¼•çŠ¶æ€ç®¡ç†
        self.is_rebuilding = False
        self._rebuild_cancelled = False
        self.progress = {
            "current": 0,
            "total": 0,
            "current_file": "",
            "percentage": 0.0
        }
        self.last_rebuild_time = None
        self.file_count = 0
        self._rebuild_lock = asyncio.Lock()

        print(f"âš™ï¸ [KnowledgeBase] åˆå§‹åŒ–ä¸­æ–‡ Embedding æ¨¡å‹ (bge-small-zh-v1.5 è½»é‡ç‰ˆ)...")
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="BAAI/bge-small-zh-v1.5",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True},
                show_progress=True  # æ˜¾ç¤ºä¸‹è½½è¿›åº¦
            )
        except Exception as e:
            print(f"âŒ [KnowledgeBase] Embedding æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e

        # åŠ è½½æˆ–é‡å»ºç´¢å¼•
        self.vector_store = self._load_or_create_index()

        # âŒ ä¸å†è‡ªåŠ¨å¯åŠ¨åå°PDFå¤„ç†ä»»åŠ¡ï¼Œæ”¹ä¸ºç”¨æˆ·æ‰‹åŠ¨è§¦å‘
        # self._pdf_task = None
        # if self.process_pdfs:
        #     self._pdf_task = asyncio.create_task(self._process_pdfs_background())

    def _load_or_create_index(self):
        # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        index_file = self.vector_db_path / "index.faiss"
        
        if index_file.exists():
            print("ğŸ“‚ [KnowledgeBase] åŠ è½½æœ¬åœ°å‘é‡ç´¢å¼• (Hit Cache)...")
            try:
                return FAISS.load_local(
                    str(self.vector_db_path), 
                    self.embeddings,
                    allow_dangerous_deserialization=True 
                )
            except Exception as e:
                print(f"âš ï¸ [KnowledgeBase] ç´¢å¼•æ–‡ä»¶æŸåï¼Œæ­£åœ¨é‡å»º: {e}")
                return self._create_index()
        else:
            print("âš™ï¸ [KnowledgeBase] æœ¬åœ°æ— ç´¢å¼•ï¼Œæ­£åœ¨é‡å»ºå‘é‡æ•°æ®åº“...")
            return self._create_index()

    def _create_index(self):
        if not self.data_path.exists():
            print(f"âš ï¸ [KnowledgeBase] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_path}ï¼Œå°†åˆ›å»ºç©ºç´¢å¼•ã€‚")
            return FAISS.from_texts(["åˆå§‹åŒ–ç©ºç™½æ–‡æ¡£"], self.embeddings)

        # 1. åŠ è½½æ–‡æ¡£
        loaders = [
            DirectoryLoader(str(self.data_path), glob="**/*.txt", loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"}),
            DirectoryLoader(str(self.data_path), glob="**/*.md", loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"}),
        ]
        
        documents = []
        for loader in loaders:
            try:
                documents.extend(loader.load())
            except Exception as e:
                print(f"âš ï¸ [KnowledgeBase] åŠ è½½æ–‡ä»¶å‡ºé”™: {e}")

        if not documents:
            print("âš ï¸ [KnowledgeBase] æœªæ‰¾åˆ°æ–‡æ¡£ï¼Œåˆ›å»ºç©ºç´¢å¼•ã€‚")
            return FAISS.from_texts(["æ— æ•°æ®"], self.embeddings)

        # 2. åˆ‡åˆ†æ–‡æ¡£ï¼ˆå¢å¤§åˆ†å—ä»¥åŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
        # ğŸ”¥ å…³é”®è°ƒæ•´ï¼šé¿å…åœ¨åˆ†å·å¤„åˆ‡åˆ†ï¼Œé˜²æ­¢äº§ç”Ÿåªæœ‰"ï¼›"çš„ç¢ç‰‡chunk
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,  # å¢åŠ åˆ° 1500 å­—ç¬¦ï¼Œç¡®ä¿åŒ…å«å®Œæ•´çš„æ®µè½
            chunk_overlap=150,  # å¢åŠ é‡å ä»¥ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
            separators=[
                "ã€‚\n",        # ä¼˜å…ˆï¼šå¥å­ç»“æŸ+æ¢è¡Œ
                "ï¼\n",        # æ„Ÿå¹å¥ç»“æŸ+æ¢è¡Œ
                "ï¼Ÿ\n",        # é—®å¥ç»“æŸ+æ¢è¡Œ
                "\n\n\n",      # ä¸‰ä¸ªæ¢è¡Œï¼ˆç« èŠ‚æ ‡é¢˜åï¼‰
                "\n\n",        # ä¸¤ä¸ªæ¢è¡Œï¼ˆæ®µè½ä¹‹é—´ï¼‰
                "\n",          # å•ä¸ªæ¢è¡Œ
                "ã€‚",          # å¥å·
                "ï¼",          # æ„Ÿå¹å·
                "ï¼Ÿ",          # é—®å·
                "ï¼Œ",          # é€—å·
                " ",          # ç©ºæ ¼
                # âŒ ç§»é™¤"ï¼›\n"å’Œ"ï¼›"ï¼Œé¿å…åœ¨åˆ†å·å¤„åˆ‡åˆ†äº§ç”Ÿæ— æ„ä¹‰chunk
                ""            # æœ€åæ‰æŒ‰å­—ç¬¦åˆ‡åˆ†
            ]
        )
        chunks = text_splitter.split_documents(documents)

        # ğŸ”¥ è¿‡æ»¤æ‰å°äº50å­—ç¬¦çš„ä½è´¨é‡chunkï¼ˆé¿å…"ï¼›"ç­‰æ— æ„ä¹‰chunkï¼‰
        original_count = len(chunks)
        chunks = [c for c in chunks if len(c.page_content) >= 50]
        filtered_count = original_count - len(chunks)
        print(f"ğŸ“„ [KnowledgeBase] åˆ‡åˆ†å‡º {original_count} ä¸ªç‰‡æ®µï¼Œè¿‡æ»¤ {filtered_count} ä¸ªå°ç‰‡æ®µï¼Œä¿ç•™ {len(chunks)} ä¸ªæœ‰æ•ˆç‰‡æ®µ...")

        # 3. åˆ›å»ºå‘é‡åº“ (å†…å­˜ä¸­)
        vector_store = FAISS.from_documents(chunks, self.embeddings)

        # 4. ä¿å­˜åˆ°æœ¬åœ° (Windows è·¯å¾„å…¼å®¹æ€§ä¿®å¤)
        try:
            # å®šä¹‰ä¸´æ—¶ç›®å½•
            temp_dir_name = "temp_faiss_build"
            temp_path = self.base_dir / temp_dir_name
            
            if temp_path.exists():
                shutil.rmtree(temp_path)

            # ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
            vector_store.save_local(str(temp_path))

            # æ¬è¿
            if self.vector_db_path.exists():
                shutil.rmtree(self.vector_db_path)
            self.vector_db_path.mkdir(parents=True, exist_ok=True)

            for file_name in os.listdir(temp_path):
                shutil.move(str(temp_path / file_name), str(self.vector_db_path / file_name))

            shutil.rmtree(temp_path)
            print(f"ğŸ’¾ [KnowledgeBase] ç´¢å¼•å·²ä¿å­˜è‡³: {self.vector_db_path}")
        except Exception as e:
            print(f"âŒ [KnowledgeBase] ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")
        
        return vector_store

    def _init_pdf_service_if_needed(self):
        """å»¶è¿Ÿåˆå§‹åŒ–PDFæœåŠ¡"""
        if self.pdf_service is None and self.process_pdfs:
            try:
                self.pdf_service = PDFService()
            except Exception as e:
                print(f"[KnowledgeBase] PDFæœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œå°†è·³è¿‡PDFå¤„ç†: {e}")
                self.process_pdfs = False

    async def _process_pdfs(self) -> List[Document]:
        """
        å¤„ç†æ‰€æœ‰PDFæ–‡ä»¶

        æµç¨‹ï¼š
        1. æ‰«ædata/knowledge/ç›®å½•ä¸‹çš„æ‰€æœ‰PDF
        2. å¯¹æ¯ä¸ªPDFï¼š
           a. è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
           b. æŸ¥è¯¢SQLiteç¼“å­˜
           c. å¦‚æœç¼“å­˜æœ‰æ•ˆ â†’ ä½¿ç”¨ç¼“å­˜
           d. å¦‚æœç¼“å­˜æ— æ•ˆ â†’ è°ƒç”¨Markeræå–
           e. ä¿å­˜ç¼“å­˜
        3. è¿”å›Documentåˆ—è¡¨

        Returns:
            List[Document]: åŒ…å«æ‰€æœ‰PDFæ–‡æœ¬çš„Documentå¯¹è±¡åˆ—è¡¨
        """
        if not self.process_pdfs:
            return []

        self._init_pdf_service_if_needed()
        if not self.pdf_service:
            return []

        # æ‰«æPDFæ–‡ä»¶
        pdf_files = list(self.data_path.glob("**/*.pdf"))

        if not pdf_files:
            print("ğŸ“‚ [KnowledgeBase] æœªå‘ç°PDFæ–‡ä»¶")
            return []

        print(f"ğŸ“„ [KnowledgeBase] å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")

        documents = []
        cache_hits = 0
        cache_misses = 0
        processing_errors = 0

        for idx, pdf_path in enumerate(pdf_files, 1):
            try:
                # ç›¸å¯¹è·¯å¾„ (ç”¨äºå­˜å‚¨)
                rel_path = str(pdf_path.relative_to(self.base_dir))
                file_name = pdf_path.name
                file_size = pdf_path.stat().st_size

                print(f"\nğŸ“„ [{idx}/{len(pdf_files)}] å¤„ç†: {file_name}")

                # 1. è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
                print(f"   è®¡ç®— SHA256 å“ˆå¸Œ...")
                file_hash = await asyncio.to_thread(
                    PDFService.calculate_file_hash,
                    str(pdf_path)
                )

                # 2. æŸ¥è¯¢ç¼“å­˜
                print(f"   ğŸ’¾ æŸ¥è¯¢ç¼“å­˜...")
                cached_doc = await self.pdf_repo.get_by_hash(file_hash)

                # 3. åˆ¤æ–­ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
                if cached_doc and cached_doc.is_valid:
                    print(f"   âœ… ç¼“å­˜å‘½ä¸­ ({cached_doc.char_count}å­—ç¬¦)")
                    cache_hits += 1
                    markdown_text = cached_doc.processed_text
                else:
                    print(f"   âš ï¸ ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨Markeræå–...")
                    cache_misses += 1

                    # è°ƒç”¨PDFæœåŠ¡æå–
                    try:
                        markdown_text, processing_time = await self.pdf_service.extract_text_async(
                            str(pdf_path),
                            validate_quality=True
                        )

                        # ä¿å­˜ç¼“å­˜
                        await self.pdf_repo.save_cache(
                            file_path=rel_path,
                            file_name=file_name,
                            file_hash=file_hash,
                            file_size=file_size,
                            processed_text=markdown_text,
                            processing_time=processing_time,
                            marker_version="0.3.2"
                        )
                        print(f"   ğŸ’¾ ç¼“å­˜å·²ä¿å­˜")

                    except PDFProcessingError as e:
                        print(f"   [PDF] å¤„ç†å¤±è´¥: {e}")
                        processing_errors += 1
                        continue

                # 4. åˆ›å»ºDocumentå¯¹è±¡
                doc = Document(
                    page_content=markdown_text,
                    metadata={
                        "source": file_name,
                        "file_path": rel_path,
                        "file_type": "pdf",
                        "char_count": len(markdown_text),
                        "file_hash": file_hash
                    }
                )
                documents.append(doc)

            except Exception as e:
                print(f"   âŒ å¤„ç†å¼‚å¸¸: {e}")
                processing_errors += 1
                continue

        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*60}")
        print(f"ğŸ“Š [KnowledgeBase] PDFå¤„ç†ç»Ÿè®¡:")
        print(f"   æ€»æ–‡ä»¶æ•°: {len(pdf_files)}")
        print(f"   ç¼“å­˜å‘½ä¸­: {cache_hits}")
        print(f"   æ–°å¤„ç†: {cache_misses}")
        print(f"   å¤„ç†å¤±è´¥: {processing_errors}")
        if len(pdf_files) > 0:
            print(f"   æˆåŠŸç‡: {((len(pdf_files)-processing_errors)/len(pdf_files)*100):.1f}%")
        print(f"{'='*60}\n")

        return documents

    async def _process_pdfs_background(self):
        """åå°å¼‚æ­¥å¤„ç†PDFä»»åŠ¡"""
        try:
            # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œè®©ä¸»æœåŠ¡å…ˆå¯åŠ¨
            await asyncio.sleep(5)

            print("âš™ï¸ [KnowledgeBase] åå°ä»»åŠ¡: å¼€å§‹å¤„ç†PDFæ–‡ä»¶...")
            pdf_docs = await self._process_pdfs()

            if pdf_docs:
                # å°†PDFæ–‡æ¡£æ·»åŠ åˆ°ç°æœ‰ç´¢å¼•
                print(f"âš™ï¸ [KnowledgeBase] æ­£åœ¨æ·»åŠ  {len(pdf_docs)} ä¸ªPDFæ–‡æ¡£åˆ°ç´¢å¼•...")

                # åˆ‡åˆ†PDFæ–‡æœ¬ï¼ˆä¼˜åŒ–åˆ†éš”ç¬¦ï¼Œé¿å…åœ¨åˆ†å·å¤„åˆ‡åˆ†ï¼‰
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1500,
                    chunk_overlap=150,
                    separators=[
                        "ã€‚\n", "ï¼\n", "ï¼Ÿ\n",
                        "\n\n\n", "\n\n", "\n",
                        "ã€‚", "ï¼", "ï¼Ÿ",
                        "ï¼Œ", " ",
                        # âŒ ç§»é™¤"ï¼›\n"å’Œ"ï¼›"ï¼Œé¿å…åœ¨åˆ†å·å¤„åˆ‡åˆ†äº§ç”Ÿæ— æ„ä¹‰chunk
                        ""
                    ]
                )
                chunks = text_splitter.split_documents(pdf_docs)

                # ğŸ”¥ è¿‡æ»¤æ‰å°äº50å­—ç¬¦çš„ä½è´¨é‡chunk
                original_count = len(chunks)
                chunks = [c for c in chunks if len(c.page_content) >= 50]
                print(f"ğŸ“„ è¿‡æ»¤: {original_count} â†’ {len(chunks)} ä¸ªchunkï¼ˆè¿‡æ»¤äº†{original_count - len(chunks)}ä¸ªå°ç‰‡æ®µï¼‰")

                # å‘é‡åŒ–
                new_vector_store = await asyncio.to_thread(
                    FAISS.from_documents,
                    chunks,
                    self.embeddings
                )

                # åˆå¹¶ç´¢å¼•
                self.vector_store.merge_from(new_vector_store)

                # ä¿å­˜ç´¢å¼•
                await asyncio.to_thread(
                    self._save_index,
                    self.vector_store
                )
                print(f"âœ… [KnowledgeBase] PDFç´¢å¼•æ›´æ–°å®Œæˆ ({len(chunks)}ä¸ªç‰‡æ®µ)")
            else:
                print("â„¹ï¸ [KnowledgeBase] æ— PDFæ–‡ä»¶éœ€è¦å¤„ç†")

        except Exception as e:
            print(f"âŒ [KnowledgeBase] PDFåå°ä»»åŠ¡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    def _save_index(self, vector_store):
        """ä¿å­˜FAISSç´¢å¼•åˆ°æœ¬åœ°"""
        try:
            # å®šä¹‰ä¸´æ—¶ç›®å½•
            temp_dir_name = "temp_faiss_build"
            temp_path = self.base_dir / temp_dir_name

            if temp_path.exists():
                shutil.rmtree(temp_path)

            # ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
            vector_store.save_local(str(temp_path))

            # æ¬è¿
            if self.vector_db_path.exists():
                shutil.rmtree(self.vector_db_path)
            self.vector_db_path.mkdir(parents=True, exist_ok=True)

            for file_name in os.listdir(temp_path):
                shutil.move(str(temp_path / file_name), str(self.vector_db_path / file_name))

            shutil.rmtree(temp_path)
            print(f"ğŸ’¾ [KnowledgeBase] ç´¢å¼•å·²ä¿å­˜è‡³: {self.vector_db_path}")
        except Exception as e:
            print(f"âŒ [KnowledgeBase] ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")

    def get_retriever(self):
        return self.vector_store.as_retriever(search_kwargs={"k": 3})

    # ğŸ”¥ğŸ”¥ğŸ”¥ ä¼˜åŒ–åçš„æœç´¢æ–¹æ³• ğŸ”¥ğŸ”¥ğŸ”¥
    async def search_with_score(self, query: str, k: int = 6):
        """
        å¼‚æ­¥æ‰§è¡Œå‘é‡æ£€ç´¢å¹¶è¿”å›çœŸå®ç›¸ä¼¼åº¦åˆ†æ•°
        """
        if not self.vector_store:
            return []

        # âœ… å…³é”®ä¼˜åŒ–ï¼šå°†åŒæ­¥çš„ FAISS æœç´¢æ”¾å…¥çº¿ç¨‹æ± ï¼Œé˜²æ­¢é˜»å¡ FastAPI ä¸»å¾ªç¯
        try:
            results = await asyncio.to_thread(
                self.vector_store.similarity_search_with_score, 
                query, 
                k=k
            )
        except Exception as e:
            print(f"âŒ [KnowledgeBase] æœç´¢å‡ºé”™: {e}")
            return []

        import math
        processed_results = []
        for doc, squared_distance in results:
            # FAISS L2 è·ç¦»è½¬æ¢ç›¸ä¼¼åº¦ç®—æ³•
            distance = math.sqrt(max(0, float(squared_distance)))
            distance = min(distance, 2.0)
            similarity = float((1 - distance / 2))
            processed_results.append((doc, similarity))

        return processed_results

    # ==========================================
    # ç´¢å¼•ç®¡ç†åŠŸèƒ½ (æ‰‹åŠ¨é‡å»ºç´¢å¼•)
    # ==========================================

    def _format_sse(self, data: dict) -> str:
        """æ ¼å¼åŒ–SSEäº‹ä»¶"""
        return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

    def _scan_knowledge_files(self) -> List[Path]:
        """æ‰«æçŸ¥è¯†åº“ç›®å½•ä¸‹çš„æ‰€æœ‰ .txt, .md å’Œ .pdf æ–‡ä»¶"""
        files = []
        if self.data_path.exists():
            files = list(self.data_path.glob("**/*.txt")) + list(self.data_path.glob("**/*.md")) + list(self.data_path.glob("**/*.pdf"))
        return files

    async def rebuild_index_stream(self) -> AsyncGenerator[str, None]:
        """
        æµå¼é‡å»ºç´¢å¼• (SSEå“åº”)

        Yields:
            str: SSEæ ¼å¼çš„JSONäº‹ä»¶
        """
        async with self._rebuild_lock:
            if self.is_rebuilding:
                yield self._format_sse({
                    "type": "error",
                    "message": "ç´¢å¼•é‡å»ºä»»åŠ¡æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç¨åå†è¯•"
                })
                return

            self.is_rebuilding = True
            self._rebuild_cancelled = False

        try:
            # 1. åˆå§‹åŒ–äº‹ä»¶
            yield self._format_sse({
                "type": "init",
                "message": "å¼€å§‹é‡å»ºçŸ¥è¯†åº“ç´¢å¼•"
            })

            # 2. æ‰«ææ–‡ä»¶
            files = self._scan_knowledge_files()
            total_files = len(files)

            if total_files == 0:
                yield self._format_sse({
                    "type": "complete",
                    "message": "æœªæ‰¾åˆ°çŸ¥è¯†åº“æ–‡ä»¶",
                    "stats": {"total_files": 0, "total_chunks": 0}
                })
                return

            # åˆ†ç±»æ–‡ä»¶
            pdf_files = [f for f in files if f.suffix.lower() == '.pdf']
            txt_files = [f for f in files if f.suffix.lower() in ['.txt', '.md']]

            self.progress["total"] = total_files
            self.file_count = total_files

            yield self._format_sse({
                "type": "step",
                "message": f"å‘ç° {total_files} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...",
                "step": "scanning"
            })

            # 3. åŠ è½½æ–‡æ¡£
            yield self._format_sse({
                "type": "step",
                "message": "æ­£åœ¨åŠ è½½æ–‡æ¡£...",
                "step": "loading"
            })

            documents = []

            # å…ˆå¤„ç†txt/mdæ–‡ä»¶
            for idx, file_path in enumerate(txt_files, 1):
                # æ£€æŸ¥æ˜¯å¦å–æ¶ˆ
                if self._rebuild_cancelled:
                    yield self._format_sse({
                        "type": "cancelled",
                        "message": "ç´¢å¼•é‡å»ºå·²å–æ¶ˆ"
                    })
                    return

                try:
                    # æ›´æ–°è¿›åº¦
                    self.progress["current"] = idx
                    self.progress["current_file"] = file_path.name
                    self.progress["percentage"] = round((idx / total_files) * 50, 1)  # txtæ–‡ä»¶å å‰50%

                    yield self._format_sse({
                        "type": "progress",
                        "current": idx,
                        "total": total_files,
                        "current_file": file_path.name,
                        "percentage": self.progress["percentage"]
                    })

                    # åŠ è½½æ–‡æ¡£
                    loader = TextLoader(str(file_path), encoding="utf-8")
                    docs = loader.load()
                    documents.extend(docs)

                except Exception as e:
                    print(f"âš ï¸ [KnowledgeBase] åŠ è½½æ–‡ä»¶ {file_path.name} å¤±è´¥: {e}")
                    continue

            # å†å¤„ç†PDFæ–‡ä»¶
            if pdf_files and self.process_pdfs:
                yield self._format_sse({
                    "type": "step",
                    "message": f"æ­£åœ¨å¤„ç† {len(pdf_files)} ä¸ªPDFæ–‡ä»¶...",
                    "step": "processing_pdfs"
                })

                for idx, file_path in enumerate(pdf_files, 1):
                    # æ£€æŸ¥æ˜¯å¦å–æ¶ˆ
                    if self._rebuild_cancelled:
                        yield self._format_sse({
                            "type": "cancelled",
                            "message": "ç´¢å¼•é‡å»ºå·²å–æ¶ˆ"
                        })
                        return

                    try:
                        # æ›´æ–°è¿›åº¦ï¼ˆPDFæ–‡ä»¶å å50%ï¼‰
                        pdf_progress = 50 + round((idx / len(pdf_files)) * 50, 1)
                        self.progress["current"] = len(txt_files) + idx
                        self.progress["current_file"] = file_path.name
                        self.progress["percentage"] = pdf_progress

                        yield self._format_sse({
                            "type": "progress",
                            "current": len(txt_files) + idx,
                            "total": total_files,
                            "current_file": file_path.name,
                            "percentage": pdf_progress
                        })

                        # å¤„ç†PDF
                        if self.pdf_service is None:
                            from src.services.pdf_service import PDFService
                            self.pdf_service = PDFService()

                        pdf_text, _ = await asyncio.to_thread(
                            self.pdf_service.extract_text,
                            str(file_path)
                        )

                        if pdf_text and len(pdf_text.strip()) > 100:
                            doc = Document(page_content=pdf_text, metadata={"source": file_path.name})
                            documents.append(doc)

                    except Exception as e:
                        print(f"âš ï¸ [KnowledgeBase] å¤„ç†PDF {file_path.name} å¤±è´¥: {e}")
                        continue

            if not documents:
                yield self._format_sse({
                    "type": "complete",
                    "message": "æœªåŠ è½½åˆ°æœ‰æ•ˆæ–‡æ¡£",
                    "stats": {"total_files": total_files, "total_chunks": 0}
                })
                return

            # 4. åˆ‡åˆ†æ–‡æ¡£
            yield self._format_sse({
                "type": "step",
                "message": "æ­£åœ¨åˆ‡åˆ†æ–‡æ¡£...",
                "step": "splitting"
            })

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1500,
                chunk_overlap=150,
                separators=[
                    "ã€‚\n", "ï¼\n", "ï¼Ÿ\n",
                    "\n\n\n", "\n\n", "\n",
                    "ã€‚", "ï¼", "ï¼Ÿ",
                    "ï¼Œ", " ",
                    ""
                ]
            )

            chunks = text_splitter.split_documents(documents)
            original_count = len(chunks)
            chunks = [c for c in chunks if len(c.page_content) >= 50]
            filtered_count = original_count - len(chunks)

            # 5. å‘é‡åŒ–ï¼ˆæ‰‹åŠ¨å®ç°ä»¥å‘é€è¿›åº¦ï¼‰
            from langchain_community.docstore.in_memory import InMemoryDocstore
            import faiss

            # è®¡ç®—embeddings
            batch_size = 100
            all_embeddings = []
            total_chunks = len(chunks)
            total_batches = (total_chunks + batch_size - 1) // batch_size

            yield self._format_sse({
                "type": "embedding_start",
                "message": f"æ­£åœ¨å‘é‡åŒ– {len(chunks)} ä¸ªç‰‡æ®µ...",
                "total_chunks": total_chunks,
                "total_batches": total_batches
            })

            for batch_num, i in enumerate(range(0, total_chunks, batch_size), 1):
                batch = chunks[i:i + batch_size]
                batch_texts = [doc.page_content for doc in batch]

                # åœ¨çº¿ç¨‹æ± ä¸­è®¡ç®—embeddings
                batch_embeddings = await asyncio.to_thread(
                    self.embeddings.embed_documents,
                    batch_texts
                )
                all_embeddings.extend(batch_embeddings)

                # å‘é€è¿›åº¦ï¼ˆæ‰¹æ¬¡å·ï¼‰
                progress = min(100, round((i + len(batch)) / total_chunks * 100, 1))
                yield self._format_sse({
                    "type": "embedding_progress",
                    "batch_num": batch_num,
                    "total_batches": total_batches,
                    "percentage": progress
                })

            # åˆ›å»ºFAISSç´¢å¼•
            embedding_dim = len(all_embeddings[0])
            index = faiss.IndexFlatL2(embedding_dim)
            index.add(np.array(all_embeddings).astype('float32'))

            # åˆ›å»ºdocstore
            docstore = InMemoryDocstore(
                {i: doc for i, doc in enumerate(chunks)}
            )
            index_to_docstore_id = {i: i for i in range(len(chunks))}

            # åˆ›å»ºVectorStore
            vector_store = FAISS(
                index=index,
                docstore=docstore,
                index_to_docstore_id=index_to_docstore_id,
                embedding_function=self.embeddings.embed_query
            )

            # 6. ä¿å­˜ç´¢å¼•
            yield self._format_sse({
                "type": "step",
                "message": "æ­£åœ¨ä¿å­˜ç´¢å¼•...",
                "step": "saving"
            })

            await asyncio.to_thread(
                self._save_index,
                vector_store
            )

            # æ›´æ–°å½“å‰å‘é‡åº“
            self.vector_store = vector_store
            self.last_rebuild_time = asyncio.get_event_loop().time()

            # 7. å®Œæˆäº‹ä»¶
            yield self._format_sse({
                "type": "complete",
                "message": "ç´¢å¼•é‡å»ºå®Œæˆ",
                "stats": {
                    "total_files": total_files,
                    "txt_files": len(txt_files),
                    "pdf_files": len(pdf_files),
                    "total_chunks": len(chunks),
                    "filtered_chunks": filtered_count
                }
            })

        except Exception as e:
            import traceback
            traceback.print_exc()
            yield self._format_sse({
                "type": "error",
                "message": f"ç´¢å¼•é‡å»ºå¤±è´¥: {str(e)}"
            })

        finally:
            async with self._rebuild_lock:
                self.is_rebuilding = False
                self._rebuild_cancelled = False

    def cancel_rebuild(self):
        """å–æ¶ˆç´¢å¼•é‡å»ºä»»åŠ¡"""
        self._rebuild_cancelled = True